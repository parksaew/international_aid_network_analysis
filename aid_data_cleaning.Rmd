---
title: "panel_data_building"
author: "Saewon Park"
date: "April 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse,
               countrycode,
               haven,
               readxl,
               plm,
               zoo,
               bigrquery,
               googleCloudStorageR,
               DAMisc,
               splines) 


lag <- dplyr::lag
lead <- dplyr::lead

```



```{r big query, echo=FALSE}

#bigrquery only supports uploading tables up to 1 GB. crspanel is 2.5 GB so I will export it as a csv and upload it using the UI
  #bigQueryR is another package that connects R to BigQuery, and seems to allow bigger uploads. Could be something to consider in the future

#it also seems to be better practice to upload to Google Cloud Storage and then into BigQuery. The R package for this is googleCloudStorageR

#current set-up:
  #crspanel exported as crspanel.csv to ./processed_data
  #crspanel uploaded to GCS in large_processed_data bucket
  #crspanel dataset pushed as a table in BigQuery (linking GCS and BigQuery)
  #crspanel data to be accessed via BigQuery for analysis

#resources:
  #https://cran.r-project.org/web/packages/bigrquery/bigrquery.pdf
  #https://www.rdocumentation.org/packages/bigrquery/versions/1.2.0
  #uploading: https://rdrr.io/cran/bigrquery/man/api-table.html
  #bigQueryR (different package) uploading: https://rdrr.io/cran/bigQueryR/man/bqr_upload_data.html
  #googleCloudStorage: http://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html 

#bigquery authentication
bq_auth(use_oob = TRUE) #not sure if I have to keep doing this

gcp_project <- "intl-aid-network-analysis"


#test to see that the connection works
aa <- bq_project_query(gcp_project, "SELECT Year FROM analysis_input.crs_combined LIMIT 5")
bb <- bq_table_download(aa, max_results = 5)
#works

```

# Building a Panel dataset 

## Introduction

The purpose of this research is to examine factors that affect international aid, including UN General Assembly voting patterns as well as diaspora activism.

This script prepares data on aid (dependent variable) and suspected predictors of aid (independent variables) for analysis. 


## Data Used

Time range: 1990-2014. This is the biggest range of years based on the availability of data.

Country range: All possible countries where data is available


Main dataset: CRS dataset of international aid data collected by the OECD (1973-2016)

  - Information about this dataset can be found here: http://www.oecd.org/dac/stats/crsguide.htm
  - This data was filtered a timeframe of 1990-2014
  - This data was converted into 2016 USD for all observations


Other datasets:

  - Ideal Points dataset of UN General Assembly voting distances between countries (1946-2014)
    - Has adjusted ideal points and affinity data (s2un, s3un) as well as absolute distances (absidealdiff) 
      - Affinity measures similarity while distance measures dissimilarity
    - Refer to codebook "Ideal_Points_Codebook.pdf"
    
  - Polity IV dataset of democracy level (1800-2016)
    - Refer to codebook "https://www.systemicpeace.org/inscr/p4manualv2016.pdf"
    
  - Penn World Tables (V 9.0) (1950-2014)
    - Information about the data can be found here "https://www.rug.nl/ggdc/productivity/pwt/pwt-releases/pwt9.0"
      - More information about each economic variable can be found here "https://www.rug.nl/ggdc/productivity/pwt/related-research-papers/the_next_generation_of_the_penn_world_table.pdf"
    
  - Uppsala Conflict Data Program - Conflict Onset (1946-2014)
    - Refer to codebook "https://ucdp.uu.se/downloads/onset/UCDP_Onset_Dataset_Codebook_19.1.pdf" (note: codebook is for an updated version of the dataset)
    
  - World Development Index (1960-2017) & Paxton's Women in Parliament (1893 - 2017)
    - For inflation and gender ratio in governments seats
    - WDI: Get metadata by going to "http://databank.worldbank.org/data/reports.aspx?source=world-development-indicators#" and looking up the information for the following series: Inflation, GDP deflator: linked series (annual %), Inflation, GDP deflator (annual %), Inflation, consumer prices (annual %), Proportion of seats held by women in national parliaments (%)
    - Paxton: Refer to codebook "https://static1.squarespace.com/static/5b0c18c9aa49a1831d52aa12/t/5b6dfb47562fa78d36007b21/1533934408284/Paxton+Green+Hughes+2008_WIP_ICPSR_Codebook.pdf"
    
  - World Bank - Unemployment (1959 - 2017)
    - Information about the data can be found through the "Details" button on https://data.worldbank.org/indicator/SL.UEM.TOTL.ZS 
    
  - CEPII - Geo Distance dyadic data on distance between countries (225 countries)
    - Refer to codebook "http://www.cepii.fr/PDF_PUB/wp/2011/wp2011-25.pdf" 
    
  - DESTA - International treaties (1948 - 2017)
    - Refer to codebook "https://www.designoftradeagreements.org/media/filer_public/b3/29/b329ac4e-146b-47bb-97ba-cab240474654/explanatory_notes19.pdf"
    
  - OECD - Immigration (1975 - 2015)
    - Only includes movement into OECD countries 
    - Information (but no metadata) about the data can be found on "https://stats.oecd.org/Index.aspx?DataSetCode=MIG"
    
  - IMF - Trade (1980 - 2015)
    - Information about the data can be found on "http://data.imf.org/?sk=9D6028D4-F14A-464C-A2F2-59B2CD424B85"
      - The link to data has changed: go to the "Query" section to retrieve data
      
  - USAID - Greenbook (Overseas loans and grants) (1946 - 2016)
    - This data was used to get measures of US military aid
    - Information about the data can be found on "https://explorer.usaid.gov/reports#tab-u.s.-overseas-loans-and-grants-(greenbook)"


connect to bigquery
https://cloud.google.com/ai-platform/notebooks/docs/use-r-bigquery


### 1. Importing and cleaning the CRS dataset from the OECD

```{r}
##Import CRS data

#get the list of CRS csv files using file name patterns of the CRS data files
crs_files <- list.files(path = "./raw_data/oecd",
                        pattern = ".*CRS.*data\\.txt",
                        full.names = T)

#read each file listed in crs_files and combine the dataset as a list
crs_data <- lapply(crs_files,
                   read.csv,
                   row.names = NULL,
                   sep = "|", 
                   quote = "",
                   stringsAsFactors = F) %>% #note: originally, I included quote = "" for pre-2010 (including 2010) files, but I'm not sure why. (if this doesn't cause issues, remove this note)
  lapply(.,
         function(x){x["X.interest1."] <- NULL; x})
  

#let's check whether all the datasets have the same set of columns
crs_column_names <- sapply(crs_data, names)
all(apply(crs_column_names,
          2,
          identical,
          crs_column_names[,1])) # should be TRUE

#binding all the separate data frames together into a list
crspanel <- bind_rows(crs_data) %>%
  rename_all(~ str_sub(., 3, -2)) #change column name format from "X.variableName." to "variableName" via string subsetting


#deleting the list from the environment because it is very large
rm(crs_data)

#saving the resulting single datafram as a csv so that this slow process does not have to be repeated
#write.csv(crspanel, "./processed_data/crspanel.csv", row.names = F) #commented out because this is memory intensive

```



```{r}

## Clean CRS data
  #Note: the warning for countrycode() shows that the unmatched "countries" are only very small islands/territories or non-country entities (regions, donor organizations)

#1. Convert all aid amount values to 2016 constant USD. For this we need to create a table of deflators for all years. Using deflators to convert current (at the time of record) values to constant values takes out the variation that comes from inflation and exchange rates. We don't need to worry about exchange rate because the amounts are already in USD 
  #The deflator data comes from OECD (https://stats.oecd.org/Index.aspx?DataSetCode=DACDEFL)
  #"Unless otherwise stated, aid activity data are expressed in United States dollars (USD) at the exchange rate prevailing in the year of the flow i.e. in current dollars." - https://www.oecd.org/dac/stats/crsguide.htm

#deflators in 2016 constant USD
deflators <- read_xls("./raw_data/oecd/Deflators-base-2016.xls", 
                        skip = 2) %>%
  rename(country_name = 1) %>%
  select(-ncol(.)) %>% #removing the last column which just repeats the country name
  filter(country_name == "United States") %>%
  gather(year, #gather the dataframe so that the columns are country_name, year, deflator
         deflator, 
         -country_name) %>%
  mutate(deflator = deflator/100, #the deflator shows value of $100 in 2016 constant USD. change the baseline to $1.
         year = as.numeric(year))



#2. Combine deflators and clean the CRS dataset 

crs_panel_clean <- crspanel %>%
  mutate(cow_donor = countrycode(DonorName, #add donor country COW codes to enable merging with other country-level datasets
                                 "country.name",
                                 "cown",
                                 warn = T),
         cow_recipient = case_when(RecipientName == "Serbia" ~ 345L, #Serbia coded separately due to Yugoslavia duplication
                                   TRUE ~ countrycode(RecipientName, #add recipient country COW codes.
                                                      "country.name",
                                                      "cown",
                                                      warn = T)),
         aid_type = case_when(SectorCode %in% c(100:140, 152, 160) ~ "social_aid", #source for categorization? Check paper
                             SectorCode %in% c(150,151) ~ "democracy_aid",
                             SectorCode %in% c(200:250) ~ "econ_capacity_aid",
                             SectorCode %in% c(300:332) ~ "production_aid",
                             SectorCode %in% c(500:600) ~ "program_aid",
                             SectorCode %in% c(720:740) ~ "disaster_aid",
                             SectorCode == 998 ~ "unspecified_aid",
                             TRUE ~ "other_aid")) %>% #I verified that all channels (sub-sectors) belonged to some aid sector
  left_join(select(deflators, #add the deflators to use for conversion later
                   -country_name), 
            by = c("Year" = "year")) %>%
  filter(!is.na(aid_type) &
           SectorCode != 998 & #998 is for unspecified aid
           SectorName != "" & #one field name was left blank (only occurred 10 times)
           !is.na(cow_donor) & 
           !is.na(cow_recipient)) %>% #Filter out all the non-countries
  select(Year,
         cow_donor,
         cow_recipient,
         aid_type,
         usd_disbursement,
         usd_commitment,
         deflator) %>%
  rename(year = Year,
         ccode1 = cow_donor,
         ccode2 = cow_recipient,
         disburse = usd_disbursement, # so only usd deflators should be used******
         commit = usd_commitment) %>%
  mutate(disburse = disburse/deflator, #converting to 2016 constant USD
         commit = commit/deflator) %>%
  group_by(ccode1, ccode2, year, aid_type) %>%
  summarise(disburse = sum(disburse, na.rm = T),
            commit = sum(commit, na.rm = T)) %>%
  pivot_wider(names_from = aid_type, #spread data s.t each combination of aid type by disbursement/commitment has a column
              values_from = c(disburse,
                              commit),
              names_sep = "_") %>%
  ungroup() %>%
  group_by(ccode1,
           ccode2) %>%
  complete(year = 1973:2016) %>% #creates rows for "missing" years - we assume that there was no aid in the missing years
  replace(is.na(.), 0) #fills in the values for the rows that were just created


#deleting the crspanel df from the environment because it is very large. Can be queried through bigrquery
rm(crspanel)

#crscollapsed <- filter(crscollapsed, year > 1989 & year < 2015) #VERIFY if this is still needed

```



### 2. Importing and cleaning the Ideal Points Dataset


```{r}

## Import Ideal Points data
ideal_points <- read.table("./raw_data/unga/Dyadicdata.tab",
                           sep="\t", 
                           header=TRUE)

## Clean Ideal Points data
ideal_points_clean <- select(ideal_points, 
                             ccode1, 
                             ccode2, 
                             year, 
                             absidealdiff) %>%
  rename(ipdiff = absidealdiff) %>% #absidealdiff measures the dissimilarity or distance between two countries
  group_by(ccode1, 
           ccode2) %>%
  complete(year = 1946:2014) %>% #creates rows for "missing" years. The NA do mean missing data here so I don't impute values 
  arrange(ccode1, 
          ccode2, 
          year) %>% #arranging rows chronologically to calculate year-to-year changes and lags
  mutate(ip_fd = ipdiff - lag(ipdiff, order_by = year), #first difference
         ipdiff_1 = lag(ipdiff, order_by = year), # Ideal points diff- 1 yr, 3 yr, 5 yr (Savun's research uses 5 yr)
         ipdiff_3 = lag(ipdiff, order_by = year, n = 3),
         ipdiff_5 = lag(ipdiff, order_by = year, n = 5), # Similarly, ideal points FD- 1 yr, 3 yr, 5 yr
         ip_fd_1 = lag(ip_fd, order_by = year),
         ip_fd_3 = lag(ip_fd, order_by = year, n = 3),
         ip_fd_5 = lag(ip_fd, order_by = year, n = 5)) 

#write.csv(ideal_points_clean, "./processed_data/ideal_points_expanded.csv", row.names = F) #commented out because this is memory intensive


```



### (temporary checkpoint) CRS and I.P. MERGE 

```{r}

crs_ip <- left_join(crs_panel_clean,
                    ideal_points_clean, 
                    by = c("ccode1", "ccode2", "year"))

```



### 3. Add data for controls from previous project

Because I did this analysis twice - first for ORGB 708 and then for my MA essay - there were multiple data cleaning files. For controls, the MA essay version had import work from the ORGB 708 paper. Instead just importing the cleaned data file, I inserted the code that created the controls DF below and cleaned it so that the entire data cleaning process would happen in one script.



Start of ORGB 708 code

```{r}



#To build a dataset of controls, We will merge several country-year datasets together. First, we create a skeleton of just the index variables and use it as the left table of the left joins that are implemented.
controls_index <- codelist_panel %>% #country-year DF from the countrycode package
  select(country.name.en,
         country.name.en.regex,
         year,
         cow.name,
         cown,
         iso3c,
         un,
         region)
  

```




```{r polity}

#polityIV
#1800-2016

polity_4v2016 <- read_excel("./raw_data/polity_4v2016.xls")

#Note that the dataset authors discourage the use of the polity metric because they believe that autocracy and democracy don't necessarily cancel out.
polity_year_uncorrected <- polity_4v2016 %>%
  select(ccode, #country code is in CoW
         country, 
         year, 
         democ, #measure of democracy
         autoc, #measure of autocracy
         polity, #democracy - autocracy.
         polity2) %>% #This revised polity version (2) fixes values like -77
  mutate(polity = ifelse(polity < -10, 
                         NA,
                         polity)) #the polity version 1 has values like -66, so let's make them NAs
#because we are going to apply lags and create variables for differences, we need to make sure that all the years are present without gaps
  

#function to check that a vector consists of all whole numbers between the smallest and biggest values present
check_missing_sequence <- function(x) {
  missing_seq <- setdiff(min(x):max(x), x)
  if(length(missing_seq) > 0) {
    return(missing_seq)
  } else {
    return(NULL)
  }
}
  
#function that takes a list of length 1 from the list of vectors (because we need to use [] instead of [[]] to preserve the vector name) and generates a dataframe with the list element name (which is the ccode in this case) as the first column ("index1") and the vector content as the second column ("index2"). This can be used as an index to insert into DFs.
list_2_indices <- function(x) {
  index_df <- data.frame(index1 = names(x),
                         x) %>%
    rename(index2 = 2)
  return(index_df)
}


#create a list of vectors of missing years for each ccode (if there are missing years)
polity_missing_year_list <- lapply(with(polity_year_uncorrected,
                                        split(year,
                                              ccode)),
                                   check_missing_sequence) %>%
  compact()


#create an empty DF to fill in with missing ccode-year combinations in the following for-loop
polity_missing_indices <- data.frame(ccode = integer(),
                                     year = integer())

#fill in the empty df polity_missing_indices with missing ccode-year combinations using list_2_indices for each 
for (i in seq(1:length(polity_missing_year_list))) {
  country_year_list <- polity_missing_year_list[i]
  new_country_df <- list_2_indices(country_year_list) %>%
    rename(ccode = index1,
           year = index2) %>%
    mutate(ccode = as.numeric(as.character(ccode)))
  polity_missing_indices <- bind_rows(polity_missing_indices,
                                      new_country_df)
}

polity_clean <- polity_year_uncorrected %>%
  full_join(polity_missing_indices,
            by = c("ccode", "year")) %>% #adding missing years so that the year sequences are gapless
  arrange(ccode,
          year) %>%
  fill(country) %>% #filling in missing "country" values with previous value, other missing variables should remain as NAs
  group_by(ccode) %>%
  mutate(polity1_dem_ord = polity - lag(polity, 2), #ordinal values for democratization as change compared to 2 years prior
         polity1_dem = as.numeric(polity1_dem_ord >= 3), #turning polity1_dem_ord as a dummy variable (same threshold as Savun's)
         polity1_squared = (polity)^2, #squared since the impact of democracy may not be linear (extremes may not get aid)
         polity2_dem_ord = polity2 - lag(polity2, 2), #same for polity2
         polity2_dem = as.numeric(polity2_dem_ord >= 3),
         polity2_squared = (polity2)^2)
  

```



```{r penn}


#Penn World Tables (version 9.0): 
  # years: 1950 - 2014, 182 countries
#Relevant variables: GDP (real gdp from expensidure side), GDP growth, population, donor GDP (as defined by Savun's paper)
  #GDP is in constant 2005 USD (https://www.rug.nl/ggdc/productivity/pwt/related-research-papers/the_next_generation_of_the_penn_world_table.pdf)

pwt90 <- read_excel("./raw_data/pwt90.xlsx", 
                    sheet = "Data")


pennwt_clean <- pwt90 %>%
  arrange(countrycode,
          year) %>% #checked that no years were missing using the custom function check_missing_sequence
  group_by(countrycode) %>% #since we will be using lag to calculate gdp growth
  mutate(cowcode = countrycode(countrycode, #here, countries are coded by ISO, while we use CoW. So we convert them to CoW
                               "iso3c", 
                               "cown",
                               warn = TRUE),
         rgdpe_per_capita = rgdpe/pop, #to get the gdp per capita, I just divide gdp by population (both are in millions)
         rgdpe_pc_growth = (exp(log(rgdpe_per_capita) - log(lag(rgdpe_per_capita))) - 1) * 100, #growth: get the differences of logs and exponentiate them. In the end, it is basically (a/b) - 1.
         #rgdpe_pc_growthB = ((rgdpe_per_capita - lag(rgdpe_per_capita))/ lag(rgdpe_per_capita)) * 100) #alt way to calculate % growth
         rdgpe_per_capita_log = log(rgdpe_per_capita), #gdp per capita logged
         pop_log_1000 = log(pop * 1000)) %>% #population that is in 1000s and logged
  select(cowcode,
         country,
         year,
         rgdpe, #real GDP from expenditure side
         rgdpe_per_capita,
         rdgpe_per_capita_log,
         rgdpe_pc_growth,
         pop,
         pop_log_1000) %>%
  ungroup() %>%
  left_join(pwt90 %>% #we also need to get the donor gdp as it was used in Savun's paper: the logged gdp average of SWE, US, FRA
              filter(countrycode %in% c("SWE", "USA", "FRA")) %>% #this variable was calculated by using a new DF since we need the same values for all countries
              select(countrycode,
                     year,
                     rgdpe) %>%
              arrange(year) %>%
              pivot_wider(names_from = countrycode,
                          values_from = rgdpe) %>%
              mutate(savun_donor_rgdpe_log = log(rowMeans(select(., #this will only differ by year & stay the same for all countries
                                                                 SWE,
                                                                 USA,
                                                                 FRA)))) %>%
              select(-c(SWE,
                        USA,
                        FRA)), #this results a df that serves the same purpose as a subquery in SQL.
            by = "year")




```






```{r ucdp}


#Conflict Initiation dataset from Uppsala University (1946-2014)
  #Country code is called "gwno" and this is in CoW. There are 176 countries
  #Out of 3689 obs, there are only 69 cases of conflict onset and 158 cases of conflict (or new conflict) after more than 2 years of peace

ucdp_2014 <- read_csv("./raw_data/ucdp-onset-conf-2014.csv")
#View(ucdp_2014)

ucdp_clean <- ucdp_2014 %>% 
  select(year, 
         gwno, 
         incidencev414, #whether there was a conflict that year
         newconflictinyearv414, #whether there was a new conflict initiated (used to create dependent var for my other paper)
         onset2v414) %>% #whether a conflict happened after two years of no conflict (initiation or not)
  rename(cowcode = gwno,
         conflict = incidencev414,
         conflict_onset = newconflictinyearv414,
         incidence_2yr = onset2v414) %>%
  btscs(., #creates the peace years (or peace spells) data where it counts the years of peace since the last initiation (not conflict)
        event = "conflict",
        tvar = "year",
        csunit = "cowcode") %>%
  group_by(cowcode) %>% 
  arrange(cowcode,
          year) %>% #checked that no years were missing using the custom function check_missing_sequence
  bind_cols(data.frame(ns(.$spell, 
                          df=4))) %>%
  rename(peace_years = spell,
         peace_years_spline1 = X1,
         peace_years_spline2 = X2,
         peace_years_spline3 = X3,
         peace_years_spline4 = X4) %>%
  mutate(conflict_prior_year = lag(conflict), #creating a variable for prior year conflict
         peace_years_squared = peace_years^2, #adding t^2
         peace_years_cubed = peace_years^3*0.001) #adding t^3. divided by 1000 to avoid numeric instability as recommended by Carter



#Explanation: splines and time cubed

#using the peace years as a duration term, we create splines to account for the autocorrelation due to time effects
#natural splines with b-spline basis with equally-spaced knots have been recommended by some
#others say that we should adjust the knots by looking at fit- but to do this we have to construct the whole dataset, so this will come later
#time cubed is another method suggested by Carter

#natural splines, with three knots at quantiles (knots are df - 1, when df is provided). This was merged into the ucdp dataset
basis_spline <- ns(ucdp_clean$peace_years, df=4)

#as an aside, the below shows that ns() with degrees and no intercept does remove the constant as required by carter
#only basis_spline3 has an extra spline basis. basis_spline and basis_spline2 are the same (knots for basis_spline2 are from basis_spline)
#basis_spline <- (ns(ucdp_clean$peace_years, df=4))
#basis_spline2 <- (ns(ucdp_clean$peace_years, knots = c(2,15,32.75)))
#basis_spline3 <- (ns(ucdp_clean$peace_years, knots = c(2,15,32.75), intercept = T))



```




```{r wdi}
#WDI dataset: Inflation and women in parliament (%)
  #merged with Paxton's dataset for WIP for the earlier years (WDI data has spotty observation before the early 2000s)
  #Paxton data for years upto 2002, and WDI data for years from 2003


wdi <- read_csv("./raw_data/WDI_Data_1960-2017.csv")


wdi_clean <- wdi %>%
  replace(. == "..", NA) %>% #label NAs correctly
  select(-`Series Code`) %>% #series code is redundant since we have seried name
  slice(1:(n()-5)) %>% #before turning the data into tidy format, we need to remove the last 5 empty rows
  rename_at(vars(ends_with("]")), 
            ~gsub( ".{8}$" ,
                   replacement = "", 
                   x = . )) %>% #changing year column names to just contain the years in numeric form
  pivot_longer(-c(`Country Name`, #rearrange data to tidy format where observations are at the country-year level
                  `Country Code`,
                  `Series Name`),
               names_to = "year", 
               values_to = "value") %>%
  pivot_wider(names_from = `Series Name`, #each series (which represent different metrics) are assigned a column
              values_from = value) %>%
  mutate(year = as.numeric(year),
         cowcode = countrycode(`Country Code`, #convert ISO codes in the data to CoW
                               "iso3c", 
                               "cown", 
                               warn = TRUE)) %>%
  rename(consumer_inflation = `Inflation, consumer prices (annual %)`,
         women_parliament = `Proportion of seats held by women in national parliaments (%)`,
         country = `Country Name`) %>%
  mutate(consumer_inflation = as.numeric(consumer_inflation),
         women_parliament = as.numeric(women_parliament),
         isocode = `Country Code`) %>%
  select(cowcode,
         country, 
         isocode,
         year, 
         consumer_inflation, #in original code, I separated inflation and WiP, check if this needs to be done
         women_parliament)
  

#In the previous code, I created a separate dataset of the ratio of women in parliament just for donor countries since this was used as an instrumental variable. for the current version of the data cleaning script, I address this later on.

#(previous code) before we just look at the recipients, I also make a separate dataset for the donors
# wdi_donors <- wdi[wdi$`Country Code` %in% donors,] 
# wdi_donors <- wdi_donors[wdi_donors$year %in% c(2003:2016),]
# wdi_donors <- rename(wdi_donors, c("wdi_cowcode"="cowcode",
#                                    "Proportion of seats held by women in national parliaments (%)" = "women_parliament"))
# wdi_donors <- dplyr::select(wdi_donors, cowcode, year, women_parliament)




#~~~~~~


#now to paxton data 1945-2003
paxton_wip <- read_dta("./raw_data/paxton_wipdata.dta")



paxton_clean <- paxton_wip %>%
  mutate_at(vars(starts_with("P")),
            ~as.numeric(.)) %>%
  select_at(vars(contains("UNID"), #select just the country code, country name, and year columns
                 contains("COUNTRYN"),
                 starts_with("P"))) %>%
  rename_at(vars(starts_with("P")), #before turning the data into country-year level tidy data, fix the year column names 
            ~gsub( "P" ,
                   replacement = "", 
                   x = . )) %>%
  pivot_longer(-c(UNID,
                  COUNTRYN),
               names_to = "year",
               values_to = "women_parliament") %>%
  mutate(year = as.numeric(year),
         cowcode = countrycode(UNID, "un", "cown", warn = TRUE)) %>%
  rename(country = COUNTRYN,
         un_id = UNID) %>%
  replace(. == "-88" | . == "-99", NA)  #label NAs correctly (-88 and -99 are missing data)
  

#In the previous code, I created a separate dataset of the ratio of women in parliament just for donor countries since this was used as an instrumental variable. for the current version of the data cleaning script, I address this later on.
 
#(Previous code) before we just extract the recipients, we get the donor data for the IV's
# donors_cowcode <- countrycode(donors, "iso3c", "cown", warn = TRUE)
# paxton_donors <- paxton_wip[paxton_wip$paxton_cowcode %in% donors_cowcode, ]
# paxton_donors <- rename(paxton_donors, c("paxton_cowcode"="cowcode"))
# paxton_donors <-  dplyr::select(paxton_donors, cowcode, year, women_parliament)



#~~~~

# merge WDI and Paxton data

wdi_paxton_merge <- wdi_clean %>%
  filter(!is.na(cowcode)) %>%
  full_join(select(paxton_clean,
                   cowcode,
                   year,
                   women_parliament),
            by = c("cowcode",
                   "year"),
            suffix = c("_wdi",
                       "_paxton")) %>%
  mutate(women_parliament = ifelse(year < 2004, 
                                   women_parliament_paxton,
                                   women_parliament_wdi))

#For donor WiP, I don't think there is a reason to calculate averages separately as we can merge WiP for both the donor and recipient sides of the dyads. But I leave the code commented out for now (this was actually commented out before too)

#(Previous code) lastly, donor wip can also be calculated through wip data
#we need the logged average of annual sweden,us,and france 1989-2014 (then repeat the data)

#wdi_donors$women_parliament <- as.numeric(wdi_donors$women_parliament)
#create a separate dataframe just for the donors
#donor_wip <- bind_rows(paxton_donors, wdi_donors)
#rearrange and calculate averages
#donor_wip <- spread(donor_wip, cowcode, women_parliament)
#donor_wip$donor_wip <- log(rowMeans(donor_wip[,-1]))
#we use the resulting vector to attach it to the pwt dataset
#donor_wip_avg <- rep(donor_wip$donor_wip, 161)
#repeating the vector 146 (while keeping the order)
#combined_wip <- cbind(combined_wip, donor_wip_avg)


```



Unemployment data was added after the ORGB project but it is still a country level control variable, so I include it here

```{r unemployment}

wb_unemployment <- read.csv(file="./raw_data/Unemployment_WB.csv", 
                                   skip = 4,
                                   header=TRUE, 
                                   sep=",") 

wb_unemployment_clean <- wb_unemployment %>%
  select(-c(X, #empty column
            Indicator.Code,#we only picked one indicator so this information is not useful
            Indicator.Name)) %>%
  rename_at(vars(starts_with("X")), #remove the "X" in front of all the column names for the years
                 funs(substr(.,2, 5))) %>%
  pivot_longer(-c(Country.Name, 
                  Country.Code),
               names_to = "year", 
               values_to = "unemployment") %>%
  group_by(Country.Code) %>%
  mutate(year = as.integer(year),
         cowcode = countrycode(Country.Code, "iso3c", "cown")) %>% 
  ungroup()


```




Bind all the ORGB 708 controls + unemployment together into one dataframe

  - This is not necessary and all the DFs can be merged at the very end, but it does make things easier to manage

```{r}

#merge all the controls from the original ORGB 708 research

#first have a version that is not lagged (most independent variables need to be lagged in the final df for analysis)
original_controls_unlagged <- controls_index %>%
  select(country.name.en,
         cown,
         year,
         region) %>%
  filter(!is.na(cown)) %>%
  rename(country = country.name.en, #all the country names will be from the country code project (may not be the best choice)
         cowcode = cown) %>%
  left_join(polity_clean %>%
              select(-country), #removing country name columns since it will be redundant
            by = c("cowcode" = "ccode",
                   "year")) %>%
  left_join(pennwt_clean %>%
              select(-c(countrycode, #removing non-CoW country codes since it will be redundant
                        country)),
            by = c("cowcode",
                   "year")) %>%
  left_join(ucdp_clean,
            by = c("cowcode",
                   "year")) %>%
  left_join(wdi_paxton_merge %>%
              select(-c(country,
                        isocode,
                        women_parliament_wdi,
                        women_parliament_paxton)),
            by = c("cowcode",
                   "year")) %>%
  left_join(wb_unemployment_clean %>%
              select(-c(Country.Name,
                        Country.Code)),
            by = c("cowcode",
                   "year")) %>%
  group_by(cowcode) %>%
  arrange(cowcode,
          year)

original_controls <- original_controls_unlagged %>%
  mutate(year = lead(year))
  

```

END of ORGB 708 code



### 4. Merge country-level controls to dyadic data

How to add country-level control variables to the dyadic dataset

  - Recipient:
    - Prior Year Conflict
    - Change in democracy level
    - Democratization (boolean)
    - Democracy level
    - Democracy level, squared (since neither full democracies or full autocracy would attract democracy aid)
    - Population (not logged, to be used to scale aid flow. "pop" and aidflow are both in millions)
    
  - Donor:
    - Inflation
    - % of Women in parliament
    
  - Both:
    - Growth in GDP per capita
    - GDP per capita
    - Logged GDP per capita
    - Population (logged)
    - % Unemployment
  

```{r control merge}
#all of the country level controls are stored in the original_controls DF


crs_ip_country_controls <- crs_ip %>%
  left_join(original_controls %>% #merge the data for recipient countries
              select(cowcode,
                     year,
                     country,
                     conflict_prior_year,
                     pop, #this is just used to correct the aid flow. This is in millions
                     polity2_dem_ord,
                     polity2_dem,
                     polity2,
                     polity2_squared,
                     rgdpe_pc_growth,
                     rgdpe_per_capita,
                     rdgpe_per_capita_log,
                     pop_log_1000,
                     unemployment) %>%
              rename_at(vars(-c(cowcode,
                                year)),
                        ~paste0("r_",
                                .)),
            by = c("ccode2" = "cowcode", #ccode2 is for recipient countries
                              "year")) %>%
  left_join(original_controls %>% #merge the data for donor countries. 
              select(cowcode,
                     year,
                     country,
                     pop, #this is unlogged and in millions
                     rgdpe_pc_growth,
                     rgdpe_per_capita,
                     rdgpe_per_capita_log,
                     pop_log_1000,
                     consumer_inflation,
                     women_parliament,
                     unemployment) %>%
              rename_at(vars(-c(cowcode,
                                year)),
                        ~paste0("d_",
                                .)),
            by = c("ccode1" = "cowcode", #ccode1 is for donor countries
                              "year")) %>%
  mutate_at(vars(contains("_aid")), #NA created for aid corrected because of missing population data (some due to lag) 
            list(corrected = ~log(((./r_pop)*1000)+1))) %>%   
  group_by(ccode2,
           ccode1) %>%
  arrange(ccode2,
          ccode1,
          year) %>%
  mutate_at(vars(contains("_corrected")), #lags to measure the effect of previous aid amounts or FD (NEEDS VALIDATION)
            list(`1` = ~lag(.),
                 `3` = ~lag(., 
                            n = 3),
                 `5` = ~lag(.,
                            n = 5))) %>%
  mutate(unemployment_diff = d_unemployment - r_unemployment) %>% #unemployment differences- negative when donor has lower unemployment
  mutate_at(c(vars(matches("^(d_|r_)")),
              vars(-matches("country")),
              vars(unemployment_diff)), #lags to measure the effect of previous year variables or FD (NEEDS VALIDATION)
            list(`1` = ~lag(.))) #something to check - a couple examples that I checked makes it look like a lag might have been applied to recipients variables but not donor. this might be a coincidents ** NEEDS VERIFICATION

  

```



### 5. Other dyadic level controls


```{r CEPII}
###CEPII data for distance between countries (no year) 

cepii <- read_dta("./raw_data/dist_cepii.dta") 

cepii_clean <- cepii %>%
  mutate(ccode1 = countrycode(iso_o, "iso3c", "cown"),
         ccode2 = countrycode(iso_d, "iso3c", "cown"),
         dist_log = log(dist + 1),
         d_region = countrycode(ccode1, "cown", "region"),
         r_region = countrycode(ccode2, "cown", "region"),
         same_region = as.numeric(d_region == r_region))


#Notes:
  #only the small countries (islands) are missing
  #dist is the simple distance between the biggest cities
  #colony is whether the pair of countries were ever in a colonial relationship
  #distw and distwces are basically the same, but distwces is the inverse to distw
    #both are weighted distance
    #distwces shrinks as distance grows and distw is the opposite



```


DESTA data- PTA 

```{r DESTA}
#create column for number of treaties (preferential trade agreement- PTA) to be filled with 0s too after merge (?- april 19)

#database of treaties between countries
desta <- read.csv(file="./raw_data/DESTA_treaties_dyadic.csv", 
                  stringsAsFactors = F)

#in desta, each agreement is only counted once (if there is US-CAN, then there is no CAN-US for a particular agreement)
  #however, we need to have the number be consistent within regardless of direction/order of pairs because treaties are non-directional
  #so we duplicate the dataset so that if there are 3 treaties between A and B, the A-B dyad as well as the B-A dyad will show "3"


#To achieve the above, we first create a dataframe with the relevant columns and country code converted that we will duplicate
desta_simple <- desta %>%
  select(iso1,
         iso2,
         base_treaty,
         year) %>%
  mutate(ccode1 = countrycode(iso1, 
                              "iso3n",
                              "cown"),
         ccode2 = countrycode(iso2,
                              "iso3n",
                              "cown")) %>%
  filter(!(is.na(ccode1)) & !is.na(ccode2)) 

#Then we bind a duplication of the desta_simple with ccode1 and ccode2 and country the treaties grouped by ccode1 and ccode so that we will have the same count of treaties for a ccode1-ccode2 dyad and a ccode2-ccode1 dyard 
desta_clean <- rbind(desta_simple,
                     desta_simple %>% #2nd dataframe with ccode order switched
                      rename(c1 = ccode1,
                             c2 = ccode2) %>%
                      rename(ccode1 = c2, #switching the ccode orders (could be written more efficiently?)
                             ccode2 = c1)) %>%
  group_by(ccode1,
           ccode2,
           year) %>%
  summarise(pta_count = n()) %>%
  ungroup() %>%
  group_by(ccode1, 
           ccode2) %>%
  complete(year = 1948:2017) %>% #creates rows for "missing" years - we assume that there were no PTAs in the absent years
  replace(is.na(.), 0) %>% #filling in the missing values for the rows we just created with "0"
  arrange(ccode1,
          ccode2,
          year) %>%
  mutate(pta_cum_sum = cumsum(pta_count), # Cumulative count of treaties - will need for autocorrelation
         pta_cum_sum_1 = lag(pta_cum_sum),
         pta_count_1 = lag(pta_count))  

  
```




Immigration Data 

```{r immigration}

immigration <- read.csv("./raw_data/Migration_1975-2015.csv", 
                             stringsAsFactors = F)


immigration_year_uncorrected <- immigration %>%
  filter(VAR %in% c("B11", "B16")) %>% # B11 is migration and B16 is naturalization (see notes at bottom of chunk)
  select(country1 = COU, #donor country (migration inflow)
         country2 = `Ã¯..CO2`, #recipient country (migration outflow)
         year = Year,
         Value,
         VAR) %>%
  pivot_wider(names_from = VAR,
              values_from = Value) %>%
  rename(immigration = B11,
         naturalization = B16) %>%
  mutate(ccode1 = countrycode(country1, 
                              "iso3c", 
                              "cown"),
         ccode2 = countrycode(country2, 
                              "iso3c",
                              "cown")) %>%
  filter(!is.na(ccode1) & !is.na(ccode2)) %>%
  select(-c(country1,
            country2)) %>%
  arrange(ccode1,
          ccode2,
          year)

  #since we need to create lags for immigration data, I check for and fix the issue of missing years below


#create a list of vectors of missing years for each ccode pair (if there are missing years)
  #this shows that there were quite a few gaps (around 3600 observations), so there could have been a significant flaw in the research
immigration_missing_year_list <- lapply(with(immigration_year_uncorrected %>%
                                               mutate(ccode_both = paste0(ccode1, #creating a variable for pairs for split()
                                                                          "_",
                                                                          ccode2)),
                                        split(year,
                                              ccode_both)),
                                   check_missing_sequence) %>%
  compact() 


#create an empty DF to fill in with missing ccode-year combinations in the following for-loop
immigration_missing_indices <- data.frame(ccode1 = integer(),
                                          ccode2 = integer(),
                                          year = integer())

#fill in the empty df immigration_missing_indices with missing ccode-year combinations using list_2_indices for each 
for (i in seq(1:length(immigration_missing_year_list))) {
  country_year_list <- immigration_missing_year_list[i]
  new_country_df <- list_2_indices(country_year_list) %>%
    rename(ccode_both = index1,
           year = index2) %>%
    mutate(ccode_both = as.character(ccode_both)) %>%
    separate(ccode_both,
             c("ccode1",
               "ccode2"),
             sep = "_") %>%
    mutate(ccode1 = as.numeric(ccode1),
           ccode2 = as.numeric(ccode2))
  immigration_missing_indices <- bind_rows(immigration_missing_indices,
                                           new_country_df)
}



immigration_clean <- immigration_year_uncorrected %>%
  full_join(immigration_missing_indices,
            by = c("ccode1",
                   "ccode2",
                   "year")) %>%
  group_by(ccode1,
           ccode2) %>%
  arrange(ccode1,
          ccode2,
          year) %>%
  mutate_at(vars(matches("^(immigration|naturalization)")), #lags naturalization and immigration to measure effect over time
            list(`1` = ~lag(.),
                 `3` = ~lag(., 
                            n = 3),
                 roll_sum = ~rollsum(., #adding 5-year rolling sums 
                                     k = 5,
                                     fill = NA,
                                     align = "right"))) %>% #"right" looks at the preceding years only
  mutate_at(vars(matches("^(immigration)$")), #lag immigration over even longer periods of time (see notes for explanation)
            list(immigration_5 = ~lag(., # need to write out the entire name because only one column is matched (a bit inefficient)
                                      n = 5),
                 immigration_7 = ~lag(.,
                                      n = 7),
                 immigration_10 = ~lag(.,
                             n = 10))) %>%
  mutate_at(vars(matches("^(immigration|naturalization)")), #log all values
            list(log = ~log(. + 1))) %>%
  mutate_at(vars(matches("ion_log$")),
            list(roll_sum = ~rollsum(., #adding 5-year rolling sums (not sure why I put a lag just for the log**)
                                     k = 5,
                                     fill = NA,
                                     align = "right"))) %>%
  mutate(naturalization_log_roll_sum_1 = lag(naturalization_log_roll_sum))
  


#Notes
  #values are not scaled (count at single person level, not in millions, etc.)
  #all host countries are OECD members
  #filter only:
    #"inflow of foreign population by nationality" (to an OECD member), or VAR = B11
      #foreign population means people with another nationality
    #"Acquisition of nationality by country of former nationality", or VAR = B16
      #this is naturalization data 

  #I lag immigration 1, 3, and 5 years because there is an argument in Menard that immigration can lead to unemployment 
    #which can lead to aid (I don't think this is directly related to lagging, but it will be good to check in case)
  #I also did a 7 year lag (used by Jones-Correa) and a 10 year lag to proxy for immigrants who have stayed for a while and are more likely to have been naturalized. 




```



Trade Data from IMF

```{r trade}
#I use exports like DESTA did
  #use export from oecd to recipient, and then recipient to oecd
#export in dollars ** I believe we have to correct for inflation ("All data are expressed in terms of US dollars. As most of the countries report in their national currency, US dollar equivalents are obtained by converting data at period average exchange rates published in lines rf or rh on the country pages in IFS.")

trade <- read.csv(file="./raw_data/DOT_04-19-2018 19-48-46-85_panel.csv",
                      stringsAsFactors = F)


trade_year_uncorrected <- trade %>%
  select(exporting_ccode_imf = Country.Code,
         importing_ccode_imf = Counterpart.Country.Code,
         year = Time.Period,
         export_uncorrected = Goods..Value.of.Exports..Free.on.board..FOB...US.Dollars..TXG_FOB_USD.) %>% #did not correct for inflation
  mutate(exporting_ccode = countrycode(exporting_ccode_imf, #change to CoW code from IMF code
                                       "imf",
                                       "cown"),
         importing_ccode = countrycode(importing_ccode_imf,
                                       "imf",
                                       "cown")) %>% 
  filter(!is.na(exporting_ccode) & !is.na(importing_ccode)) %>%
  left_join(select(deflators,
                   -country_name),
            by = "year") %>%
  mutate(export_corrected = export_uncorrected/deflator) %>% #correcting for inflation (the data *seems* to be in current USD)
  mutate_at(vars(contains("export_")),
            list(log = ~log(. + 1)))


#since we need to create lags for trade data, I check for and fix the issue of missing years below
trade_missing_year_list <- lapply(with(trade_year_uncorrected %>%
                                      mutate(ccode_both = paste0(exporting_ccode, #creating a variable for pairs for split()
                                                                 "_",
                                                                 importing_ccode)),
                                    split(year,
                                          ccode_both)),
                               check_missing_sequence) %>%
  compact() 

#create an empty DF to fill in with missing ccode-year combinations in the following for-loop
trade_missing_indices <- data.frame(exporting_ccode = integer(),
                                    importing_ccode = integer(),
                                    year = integer())

#fill in the empty df immigration_missing_indices with missing ccode-year combinations using list_2_indices for each 
for (i in seq(1:length(trade_missing_year_list))) {
  country_year_list <- trade_missing_year_list[i]
  new_country_df <- list_2_indices(country_year_list) %>%
    rename(ccode_both = index1,
           year = index2) %>%
    mutate(ccode_both = as.character(ccode_both)) %>%
    separate(ccode_both,
             c("exporting_ccode",
               "importing_ccode"),
             sep = "_") %>%
    mutate(exporting_ccode = as.numeric(exporting_ccode),
           importing_ccode = as.numeric(importing_ccode))
  trade_missing_indices <- bind_rows(trade_missing_indices,
                                     new_country_df)
}


trade_clean <- trade_year_uncorrected %>%
  full_join(trade_missing_indices,
            by = c("exporting_ccode",
                   "importing_ccode",
                   "year")) %>%
  group_by(exporting_ccode,
           importing_ccode) %>%
  arrange(exporting_ccode,
          importing_ccode,
          year) %>%
  mutate_at(vars(contains("export_")),
            list(`1` = ~lag(.)))

#checked that there are no repeating combinations of the two ccodes and year


```


US military aid from foreign aid greenbook

```{r usaid}

#in 2016 constant dollars, obligations not disbursement

us_aid <- read_excel("./raw_data/us_foreignaid_greenbook.xlsx", 
                     skip = 6)

us_military_aid_clean <- us_aid %>%
  rename(year = `Fiscal Year`,
         country_name = Country,
         category = `Assistance Category`,
         us_military_aid = `Obligations (Constant Dollars)`) %>%
  filter(category == "Military") %>%
  select(year,
         country_name,
         us_military_aid) %>%
  mutate(ccode2 = countrycode(country_name, 
                              "country.name", 
                              "cown")) %>%
  filter(!is.na(ccode2) &
           !is.na(year)) %>%
  group_by(ccode2,
           year) %>%
  summarise(us_military_aid = sum(us_military_aid)) %>%
  ungroup() %>%
  group_by(ccode2) %>%
  complete(year = 1947:2016) %>%
  arrange(ccode2,
          year) %>%
  mutate(us_military_aid = ifelse(is.na(us_military_aid), #will be logged after calculating per person amount during merge
                                  0,
                                  us_military_aid),
         us_military_aid_1 = lag(us_military_aid),
         us_military_aid_3 = lag(us_military_aid,
                                 3),
         us_military_aid_5 = lag(us_military_aid,
                                 5))



```




### 6. Merging all data together

```{r merge}

# custom functions from https://stackoverflow.com/questions/47889573/use-perl-true-regex-in-dplyr-select because I am using perl compatible regex (with look aheads and look behinds) but dplyr's match does not have an option to activate the perl option
matches2 <- function (match, ignore.case = TRUE, vars = tidyselect::peek_vars()) 
  {
    grep_vars2(match, vars, ignore.case = ignore.case)
  }

grep_vars2 <- function (needle, haystack, ...) 
  {
    grep(needle, haystack, perl = TRUE, ...)
  }


#final dataframe
merged_dyadic <- crs_ip_country_controls %>%
  left_join(select(cepii_clean, #selecting all necessary variables instead of deselecting variables for code visibility/clarity 
                   ccode1,
                   ccode2,
                   contig, #contiguity (sharing a border), boolean
                   comlang_off, #shared official language, boolean
                   comlang_ethno, #shared language (at least 9% of population for both), boolean
                   colony, #past colonial relationship, boolean
                   dist, #distance between most populated cities, km
                   distw, #distance weighted by the share of the city in the overall country's population (for both)
                   distwces, #inverse of distw (decreases as distance increases)
                   dist_log, #log of dist
                   d_region,
                   r_region,
                   same_region),
            by = c("ccode1",
                   "ccode2")) %>%
  left_join(desta_clean,
            by = c("ccode1",
                   "ccode2",
                   "year")) %>%
  left_join(immigration_clean,
            by = c("ccode1",
                   "ccode2",
                   "year")) %>%
  left_join(select(trade_clean, #first merging for donor to receipient exports
                   -c(exporting_ccode_imf,
                      importing_ccode_imf)),
            by = c("ccode1" = "exporting_ccode",
                   "ccode2" = "importing_ccode",
                   "year")) %>%
  rename_at(vars(matches("^export_")),
            ~paste("d_to_r",
                   .,
                   sep = "_")) %>%
  left_join(select(trade_clean, #second merging for recipient to donor exports
                   -c(exporting_ccode_imf,
                      importing_ccode_imf,
                      deflator)), #since the deflator variable was added in the previous merge already
            by = c("ccode2" = "exporting_ccode",
                   "ccode1" = "importing_ccode",
                   "year")) %>%
  rename_at(vars(matches("^export_")),
            ~paste("r_to_d",
                   .,
                   sep = "_")) %>%
  left_join(us_military_aid_clean,
            by = c("ccode2",
                   "year")) %>%
  mutate_at(vars(contains("us_military_aid")),
            list(tempcor = ~log((./r_pop*1000) + 1))) %>% #correcting military aid amount by population ("tempcor" is just a placeholder)
  rename_at(vars(matches("us_military_aid.*tempcor")),
            ~gsub("^(us_military_aid)(.*)$", #renaming the new variables so that the lags are at the end of the var name
                  "\\1_corrected\\2", #substituting the "tempcor" placeholder with "corrected"
                  .) %>%
              gsub("_tempcor",
                   "",
                   .)) %>%
  mutate(dyad_id = paste(ccode1, #creating a dyad-id for augmented dickey fuller tests
                         ccode2, 
                         sep = "_")) %>%
  group_by(ccode1,
           ccode2) %>%
  arrange(ccode1,
          ccode2,
          year) %>%
  mutate(other_economic_aid = disburse_disaster_aid + #aggregation of other economic aid to compare easily to democracy aid
           disburse_econ_capacity_aid +
           disburse_other_aid + 
           disburse_production_aid +
           disburse_program_aid +
           disburse_social_aid,
         other_economic_aid_corrected = log((other_economic_aid/r_pop*1000) + 1)) %>% #same processing method as other aid types
  mutate_at(vars(contains("other_economic_aid")),
            list(`1` = ~ lag(.))) %>%
  mutate_at(vars(matches("^(immigration(_roll_sum)*|naturalization(_roll_sum)*)$")),
            list(prop_log = ~ log(./d_pop)+1)) %>% #calculates immigration/naturalization as proportional to donor pop in mil
  mutate_at(vars(matches("(aid_corrected$)|(^(r_|d_)(?!to_).*(?<!\\d)(?<!country)(?<!region)$)|(^(pta_).*(?<!\\d)$)|(^unemployment_diff$)|(^(immigration|naturalization).*(?<!sum_1)$)|(export_corrected(_log)*$)", 
                         perl = T)), #see notes below explaining the regex
            list(fd = ~(. - lag(.)), #first differences (needed for ECM, stationarity testing, and the FD model)
                 fd_lag = ~lag(. - lag(.))))  #first difference lagged


#to do TWC with HC3 errors, turn data into pdata.frame and use plm
merged_dyadic_plm <- pdata.frame(merged_dyadic,
                                 index = c("ccode1", "ccode2"))


#export as csv (warning: long execution time)
write.csv(merged_dyadic,
          "./processed_data/merged_dyadic_analysis_data.csv",
          row.names = F)


## Notes ##

#regex:
  # - matches aid variables that are not lagged (meaning the ending is always "aid_corrected")
  # - donor and recipient variables (starts with d_ or r_) that are not lagged (meaning they don't end with a number) and is not the country name (meaning they don't end with "country")
  # - pta variables (count and cumulative sums) that are not lagged (meaning they don't end with a number)
  # - difference in unemployment
  # - immigration and naturalization except for the lag of roll sums (meaning they don't end with "sum_1")
  # - corrected export values, logged and not logged. But not lagged values (meaning they only end with "corrected" or "log")


# Old notes:
  # I need to remove all missing aidflow corrected value
    #since we can't have missing Y 
      #June 2020 note *** : not sure if this is still necessary (also NAs were filled with 0s so we need to check what is still NA)
  # mergedata <- filter(mergedata, 
  #                      !is.na(aidflow_corrected) & !is.nan(aidflow_corrected) & !is.infinite(aidflow_corrected))
    #but I would not do this because we still need to lag data 
      #June 2020 note *** : not sure if this is still necessary
  #for the pdata.frame there are warning because of duplicate dyad (since I have years now)
    #let's proceed for now and see what happens


```




```{r google cloud storage}


#tutorial: http://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html
  #the auto-authentication and auto-default bucket set up is not working so I do them manually
  #upload to bucket i created earlier, but for processed data



#authenticating using the JSON file taken from my Google Project
gcs_auth("C:/Users/Spark/Desktop/McGill/Winter_2018/ORGB_708/research/researchproj/processed_data/processed_data_upload_intl_aid.json")

#setting the default bucket
gcs_global_bucket("large_processed_data")


#uploading merged_dyadic csv file to default bucket
# merged_dyadic_gcs_upload <- gcs_upload("./processed_data/merged_dyadic_analysis_data.csv",
#                                        bucket = "large_processed_data") 
  #this fails while uploading every time (seems to be an issue with the package or internet connection)

# merged_dyadic_gcs_upload #when successful this should show the metadata

#checking that uploaded object is listed under the default bucket
gcs_list_objects()

```



