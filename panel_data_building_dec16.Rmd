---
title: "panel_data_building"
author: "SaewonPark"
date: "April 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse,
               countrycode,
               haven,
               readxl,
               plm,
               RcppRoll,
               bigrquery) 


lag <- dplyr::lag
lead <- dplyr::lead

```



```{r}

#bigrquery only supports uploading tables up to 1 GB. crspanel is 2.5 GB so I will export it as a csv and upload it using the UI
  #bigQueryR is another package that connects R to BigQuery, and seems to allow bigger uploads. Could be something to consider in the future

#it also seems to be better practice to upload to Google Cloud Storage and then into BigQuery. The R package for this is googleCloudStorageR

#current set-up:
  #crspanel exported as crspanel.csv to ./processed_data
  #crspanel uploaded to GCS in large_processed_data bucket
  #crspanel dataset pushed as a table in BigQuery (linking GCS and BigQuery)
  #crspanel data to be accessed via BigQuery for analysis

#resources:
  #https://cran.r-project.org/web/packages/bigrquery/bigrquery.pdf
  #https://www.rdocumentation.org/packages/bigrquery/versions/1.2.0
  #uploading: https://rdrr.io/cran/bigrquery/man/api-table.html
  #bigQueryR uploading: https://rdrr.io/cran/bigQueryR/man/bqr_upload_data.html
  #googleCloudStorage: http://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html 

#bigquery authentication
#bq_auth(use_oob = TRUE) #not sure if I have to keep doing this

gcp_project <- "intl-aid-network-analysis"


#test to see that the connection works
#aa <- bq_project_query(gcp_project, "SELECT Year FROM crs_panel_1973_2016.original_combined_1973_2016 LIMIT 5")
#bb <- bq_table_download(aa, max_results = 5)
#works

```

# Building a Panel dataset 

## Introduction

The purpose of this research is to examine factors that affect international aid, including UN General Assembly voting patterns as well as diaspora activism.

This script prepares data on aid (dependent variable) and suspected predictors of aid (independent variables) for analysis. 


## Data Used

Time range: 1990-2014. This is the biggest range of years based on the availability of data.

Country range: All possible countries where data is available


Main dataset: CRS dataset of international aid data collected by the OECD (1973-2016)

  - Information about this dataset can be found here: http://www.oecd.org/dac/stats/crsguide.htm
  - This data was filtered a timeframe of 1990-2014
  - This data was converted into 2016 USD for all observations


Other datasets:

  - Ideal Points dataset of UN General Assembly voting distances between countries (1946-2014)
    - Has adjusted ideal points and affinity data (s2un, s3un) as well as absolute distances (absidealdiff) 
      - Affinity measures similarity while distance measures dissimilarity
    - Refer to codebook "Ideal_Points_Codebook.pdf"
    
  - Polity IV dataset of democracy level (1800-2016)
    - Refer to codebook "https://www.systemicpeace.org/inscr/p4manualv2016.pdf"
    
  - Penn World Tables (V 9.0) (1950-2014)
    - Information about the data can be found here "https://www.rug.nl/ggdc/productivity/pwt/pwt-releases/pwt9.0"


connect to bigquery
https://cloud.google.com/ai-platform/notebooks/docs/use-r-bigquery


### 1. Importing and cleaning the CRS dataset from the OECD

```{r}
##Import CRS data

#get the list of CRS csv files using file name patterns of the CRS data files
crs_files <- list.files(path = "./raw_data/oecd",
                        pattern = ".*CRS.*data\\.txt",
                        full.names = T)

#read each file listed in crs_files and combine the dataset as a list
crs_data <- lapply(crs_files,
                   read.csv,
                   row.names = NULL,
                   sep = "|", 
                   quote = "",
                   stringsAsFactors = F) %>% #note: originally, I included quote = "" for pre-2010 (including 2010) files, but I'm not sure why. (if this doesn't cause issues, remove this note)
  lapply(.,
         function(x){x["X.interest1."] <- NULL; x})
  

#let's check whether all the datasets have the same set of columns
crs_column_names <- sapply(crs_data, names)
all(apply(crs_column_names,
          2,
          identical,
          crs_column_names[,1])) # should be TRUE

#binding all the separate data frames together into a list
crspanel <- bind_rows(crs_data) %>%
  rename_all(~ str_sub(., 3, -2)) #change column name format from "X.variableName." to "variableName" via string subsetting


#deleting the list from the environment because it is very large
rm(crs_data)

#saving the resulting single datafram as a csv so that this slow process does not have to be repeated
#write.csv(crspanel, "./processed_data/crspanel.csv", row.names = F) #commented out because this is memory intensive

```



```{r}

## Clean CRS data
  #Note: the warning for countrycode() shows that the unmatched "countries" are only very small islands/territories or non-country entities (regions, donor organizations)

#1. Convert all aid amount values to 2016 constant USD. For this we need to create a table of deflators for all years. Using deflators to convert current (at the time of record) values to constant values takes out the variation that comes from inflation and exchange rates. We don't need to worry about exchange rate because the amounts are already in USD 
  #The deflator data comes from OECD (https://stats.oecd.org/Index.aspx?DataSetCode=DACDEFL)
  #"Unless otherwise stated, aid activity data are expressed in United States dollars (USD) at the exchange rate prevailing in the year of the flow i.e. in current dollars." - https://www.oecd.org/dac/stats/crsguide.htm

#deflators in 2016 constant USD
deflators <- read_xls("./raw_data/oecd/Deflators-base-2016.xls", 
                        skip = 2) %>%
  rename(country_name = 1) %>%
  select(-ncol(.)) %>% #removing the last column which just repeats the country name
  filter(country_name == "United States") %>%
  gather(year, #gather the dataframe so that the columns are country_name, year, deflator
         deflator, 
         -country_name) %>%
  mutate(deflator = deflator/100, #the deflator shows value of $100 in 2016 constant USD. change the baseline to $1.
         year = as.numeric(year))



#2. Combine deflators and clean the CRS dataset 

crs_panel_clean <- crspanel %>%
  mutate(cow_donor = countrycode(DonorName, #add donor country COW codes to enable merging with other country-level datasets
                                 "country.name",
                                 "cown",
                                 warn = T),
         cow_recipient = case_when(RecipientName == "Serbia" ~ 345L, #Serbia coded separately due to Yugoslavia duplication
                                   TRUE ~ countrycode(RecipientName, #add recipient country COW codes.
                                                      "country.name",
                                                      "cown",
                                                      warn = T)),
         aid_type = case_when(SectorCode %in% c(100:140, 152, 160) ~ "social_aid", #source for categorization? Check paper
                             SectorCode %in% c(150,151) ~ "democracy_aid",
                             SectorCode %in% c(200:250) ~ "econ_capacity_aid",
                             SectorCode %in% c(300:332) ~ "production_aid",
                             SectorCode %in% c(500:600) ~ "program_aid",
                             SectorCode %in% c(720:740) ~ "disaster_aid",
                             SectorCode == 998 ~ "unspecified_aid",
                             TRUE ~ "other_aid")) %>% #I verified that all channels (sub-sectors) belonged to some aid sector
  left_join(select(deflators, #add the deflators to use for conversion later
                   -country_name), 
            by = c("Year" = "year")) %>%
  filter(!is.na(aid_type) &
           SectorCode != 998 & #998 is for unspecified aid
           SectorName != "" & #one field name was left blank (only occurred 10 times)
           !is.na(cow_donor) & 
           !is.na(cow_recipient)) %>% #Filter out all the non-countries
  select(Year,
         cow_donor,
         cow_recipient,
         aid_type,
         usd_disbursement,
         usd_commitment,
         deflator) %>%
  rename(year = Year,
         ccode1 = cow_donor,
         ccode2 = cow_recipient,
         disburse = usd_disbursement, # so only usd deflators should be used******
         commit = usd_commitment) %>%
  mutate(disburse = disburse/deflator, #converting to 2016 constant USD
         commit = commit/deflator) %>%
  group_by(ccode1, ccode2, year, aid_type) %>%
  summarise(disburse = sum(disburse, na.rm = T),
            commit = sum(commit, na.rm = T)) %>%
  pivot_wider(names_from = aid_type, #spread data s.t each combination of aid type by disbursement/commitment has a colum
              values_from = c(disburse,
                              commit),
              names_sep = "_") %>%
  ungroup() %>%
  group_by(ccode1,
           ccode2) %>%
  complete(year = 1973:2016) %>% #creates rows for "missing" years - we assume that there was no aid in the missing years
  replace(is.na(.), 0) #fills in the values for the rows that were just created


#deleting the crspanel df from the environment because it is very large. Can be queried through bigrquery
rm(crspanel)

#crscollapsed <- filter(crscollapsed, year > 1989 & year < 2015) #VERIFY if this is still needed

```



### 2. Importing and cleaning the Ideal Points Dataset


```{r}

## Import Ideal Points data
ideal_points <- read.table("./raw_data/unga/Dyadicdata.tab",
                           sep="\t", 
                           header=TRUE)

## Clean Ideal Points data
ideal_points_clean <- select(ideal_points, 
                             ccode1, 
                             ccode2, 
                             year, 
                             absidealdiff) %>%
  rename(ipdiff = absidealdiff) %>% #absidealdiff measures the dissimilarity or distance between two countries
  group_by(ccode1, 
           ccode2) %>%
  complete(year = 1946:2014) %>% #creates rows for "missing" years. The NA do mean missing data here so I don't impute values 
  arrange(ccode1, 
          ccode2, 
          year) %>% #arranging rows chronologically to calculate year-to-year changes and lags
  mutate(ipfd = ipdiff - lag(ipdiff, order_by = year), #first difference
         ipdiff_1 = lag(ipdiff, order_by = year), # Ideal points diff- 1 yr, 3 yr, 5 yr (Savun's research uses 5 yr)
         ipdiff_3 = lag(ipdiff, order_by = year, n = 3),
         ipdiff_5 = lag(ipdiff, order_by = year, n = 5), # Similarly, ideal points FD- 1 yr, 3 yr, 5 yr
         ipfd_1 = lag(ipfd, order_by = year),
         ipfd_3 = lag(ipfd, order_by = year, n = 3),
         ipfd_5 = lag(ipfd, order_by = year, n = 5)) 

#write.csv(ideal_points_clean, "./processed_data/ideal_points_expanded.csv", row.names = F) #commented out because this is memory intensive


```



### (temporary checkpoint) CRS and I.P. MERGE 

```{r}

crs_ip <- left_join(crs_panel_clean,
                    ideal_points_clean, 
                    by = c("ccode1", "ccode2", "year"))

```



### 3. Add data for controls 

Because I did this analysis twice - first for ORGB 708 and then for my MA essay - there were multiple data cleaning files. For controls, the MA essay version had import work from the ORGB 708 paper. Instead just importing the cleaned data file, I inserted the code that created the controls DF below and cleaned it so that the entire data cleaning process would happen in one script.



Start of ORGB 708 code

```{r}

#To build a dataset of controls, We will merge several country-year datasets together. First, we create a skeleton of just the index variables and use it as the left table of the left joins that are implemented.
controls_index <- codelist_panel %>% #country-year DF from the countrycode package
  select(country.name.en,
         country.name.en.regex,
         year,
         cow.name,
         cown,
         iso3c,
         un,
         region)
  


#May27 - I think I just don't use this at all in the end
#Savun_Tirone_2011_AJPS_Replication_Dataset <- read_dta("Savun-Tirone 2011 AJPS Replication Dataset.dta")
#savun_data_base <- Savun_Tirone_2011_AJPS_Replication_Dataset

#years I am interested in
years <- c(1989:2016)

#to remove empty vectors
remove <- c("")

#get a vector of the countries that are included
#countries<- unique(Savun_Tirone_2011_AJPS_Replication_Dataset$recipient_country)
#countries <- countries[!countries == ""]
countries <- countries_chr


#deleting NAs

#cow_code <- unique(Savun_Tirone_2011_AJPS_Replication_Dataset$recipient_cowcode)cow_code <- countries_cow



### I changed the number of countries in the different sections 

```




```{r}

#polityIV
#1800-2016

polity_4v2016 <- read_excel("./raw_data/polity_4v2016.xls")

#Note that the dataset authors discourage the use of the polity metric because they believe that autocracy and democracy don't necessarily cancel out.
polity_year_uncorrected <- polity_4v2016 %>%
  select(ccode, #country code is in CoW
         country, 
         year, 
         democ, #measure of democracy
         autoc, #measure of autocracy
         polity, #democracy - autocracy.
         polity2) %>% #This revised polity version (2) fixes values like -77
  mutate(polity = ifelse(polity < -10, 
                         NA,
                         polity)) #the polity version 1 has values like -66, so let's make them NAs
#because we are going to apply lags and create variables for differences, we need to make sure that all the years are present without gaps
  

#function to check that a vector consists of all whole numbers between the smallest and biggest values present
check_missing_sequence <- function(x) {
  missing_seq <- setdiff(min(x):max(x), x)
  if(length(missing_seq) > 0) {
    return(missing_seq)
  } else {
    return(NULL)
  }
}
  
#function that takes a list of length 1 from the list of vectors (because we need to use [] instead of [[]] to preserve the vector name) and generates a dataframe with the list element name (which is the ccode in this case) as the first column ("index1") and the vector content as the second column ("index2"). This can be used as an index to insert into DFs.
list_2_indices <- function(x) {
  index_df <- data.frame(index1 = names(x),
                         x) %>%
    rename(index2 = 2)
  return(index_df)
}


#create a list of vectors of missing years for each ccode (if there are missing years)
polity_missing_year_list <- lapply(with(polity_year_uncorrected,
                                        split(year,
                                              ccode)),
                                   check_missing_sequence) %>%
  compact()


#create an empty DF to fill in with missing ccode-year combinations in the following for-loop
polity_missing_indices <- data.frame(ccode = integer(),
                                     year = integer())

#fill in the empty df polity_missing_indices with missing ccode-year combinations using list_2_indices for each 
for (i in seq(1:length(polity_missing_year_list))) {
  country_year_list <- polity_missing_year_list[i]
  new_country_df <- list_2_indices(country_year_list) %>%
    rename(ccode = index1,
           year = index2) %>%
    mutate(ccode = as.numeric(as.character(ccode)))
  polity_missing_indices <- bind_rows(polity_missing_indices,
                                      new_country_df)
}

polity_clean <- polity_year_uncorrected %>%
  full_join(polity_missing_indices,
            by = c("ccode", "year")) %>% #adding missing years so that the year sequences are gapless
  arrange(ccode,
          year) %>%
  fill(country) %>% #filling in missing "country" values with previous value, other missing variables should remain as NAs
  group_by(ccode) %>%
  mutate(polity1_dem_ord = polity - lag(polity, 2), #ordinal values for democratization as change compared to 2 years prior
         polity1_dem = as.numeric(polity1_dem_ord >= 3), #turning polity1_dem_ord as a dummy variable (same threshold as Savun's)
         polity2_dem_ord = polity2 - lag(polity2, 2), #same for polity2
         polity2_dem = as.numeric(polity2_dem_ord >= 3))
  

```



```{r}


#Penn World Tables (version 9.0): 
  # years: 1950 - 2014, 182 countries
#Relevant variables: GDP (real gdp from expensidure side), GDP growth, population, donor GDP (as defined by Savun's paper)

pwt90 <- read_excel("./raw_data/pwt90.xlsx", 
                    sheet = "Data")


pennwt_clean <- pwt90 %>%
  arrange(countrycode,
          year) %>% #checked that no years were missing
  group_by(countrycode) %>% #since we will be using lag to calculate gdp growth
  mutate(cowcode = countrycode(countrycode, #here, countries are coded by ISO, while we use CoW. So we convert them to CoW
                               "iso3c", 
                               "cown",
                               warn = TRUE),
         rgdpe_per_capita = rgdpe/pop, #to get the gdp per capita, I just divide gdp by population (both are in millions)
         rgdpe_pc_growth = (exp(log(rgdpe_per_capita) - log(lag(rgdpe_per_capita))) - 1) * 100, #growth: get the differences of logs and exponentiate them. In the end, it is basically (a/b) - 1.
         #rgdpe_pc_growthB = ((rgdpe_per_capita - lag(rgdpe_per_capita))/ lag(rgdpe_per_capita)) * 100) #alt way to calculate % growth
         pop_log_1000 = log(pop * 1000)) %>% #population that is in 1000s and logged
  select(cowcode,
         country,
         year,
         rgdpe, #real GDP from expenditure side
         rgdpe_per_capita,
         rgdpe_pc_growth,
         pop,
         pop_log_1000) %>%
  ungroup() %>%
  left_join(pwt90 %>% #we also need to get the donor gdp as it was used in Savun's paper: the logged gdp average of SWE, US, FRA
              filter(countrycode %in% c("SWE", "USA", "FRA")) %>% #this variable was calculated by using a new DF since we need the same values for all countries
              select(countrycode,
                     year,
                     rgdpe) %>%
              arrange(year) %>%
              pivot_wider(names_from = countrycode,
                          values_from = rgdpe) %>%
              mutate(savun_donor_rgdpe_log = log(rowMeans(select(.,
                                                                 SWE,
                                                                 USA,
                                                                 FRA)))) %>%
              select(-c(SWE,
                        USA,
                        FRA)),
            by = "year")




```






```{r}

#conflict initiation
#the cow codes are also used here (called gwno)

ucdp_2014 <- read_csv("ucdp-onset-conf-2014.csv")
#View(ucdp_2014)

ucdp_onset_2014 <- ucdp_2014

#let's choose the countries that we are interested in
ucdp_onset_2014 <- ucdp_onset_2014[ucdp_onset_2014$gwno %in% cow_code,]
#keep the years for now so that we can create peace years

#check the number of countries
length(unique(ucdp_onset_2014$gwno))
#there are 176 countries 

#out of 3689 obs, there are only 69 cases of onset and 158 cases of conflict (or new conflict) after more than 2 years of peace
#so I will keep both variables just in case
table(ucdp_onset_2014$newconflictinyearv414)
table(ucdp_onset_2014$onset2v414)
table(ucdp_onset_2014$incidencev414)

#let's select the variables we need
#incidencev412 is whether there is a conflict, 
#onset2cv412 is whether a conflict happened after two years of no conflict (can be initiation or not)
#newconflictinyearv412 is whether there was a new conflict initiated (what we are looking for to create dependent)
ucdp_onset_2014 <- dplyr::select(ucdp_onset_2014, year, gwno, incidencev414, newconflictinyearv414, onset2v414)

#conflict initiation data used is tricky (the name is misleading too)
#it is a measure of conflict incidence that happen after at least two years since a initiation (as defined by UCDP)
#incidence = 1 & conflict_ini (for previous year)= 0 & conflict_ini (for two years prior) = 0
#I lag the conflict initiation data twice (one for 1 year lag, other for 2 year lag)
#ucdp_onset_2014 <- slide(ucdp_onset_2014, Var = "newconflictinyearv414", GroupVar = "gwno", slideBy = -1)
#ucdp_onset_2014 <- slide(ucdp_onset_2014, Var = "newconflictinyearv414", GroupVar = "gwno", slideBy = -2)

#create prior year conflict variable by lagging incidencev414
ucdp_onset_2014 <- slide(ucdp_onset_2014, Var = "incidencev414", GroupVar = "gwno", slideBy = -1, reminder = F)

#creating our conflict initiation (defined by savun) data
#ucdp_onset_2014$conflict_ini <- 0
#ucdp_onset_2014$conflict_ini[which(ucdp_onset_2014$newconflictinyearv414==1 & #ucdp_onset_2014$`newconflictinyearv414-1`==0 &
#                                     ucdp_onset_2014$`newconflictinyearv414-2`==0)] <- 1

#table(ucdp_onset_2014$conflict_ini)
#remove the lagged vectors
#ucdp_onset_2014$`newconflictinyearv414-1` <- NULL
#ucdp_onset_2014$`newconflictinyearv414-2` <- NULL

#need to make peace years data, this counts the years of peace since the last initiation (not conflict)
#the btscs function creates a new df with the peace years in it as "spell"
ucdp_onset_2014_pyears <- btscs(ucdp_onset_2014, "incidencev414", "year", "gwno")

#I attached the spell column to our ucdp dataset
ucdp_onset_2014$peaceyears <- ucdp_onset_2014_pyears$spell

#now we can select our years of interest
ucdp_onset_2014 <- ucdp_onset_2014[ucdp_onset_2014$year %in% years,]


#splines and time cubed

#using the peace years as a duration term, we create splines to account for the autocorrelation due to time effects
#natural splines with b-spline basis with equally-spaced knots have been recommended by some
#others say that we should adjust the knots by looking at fit- but to do this we have to construct the whole dataset, so this will come later
#time cubed is another method suggested by carter

#natural splines, with three knots at quantiles (knots are df - 1, when df is provided)
basis_spline <- (ns(ucdp_onset_2014$peaceyears, df=4))

#as an aside, the below shows that ns() with degrees and no intercept does remove the constant as required by carter
#only basis_spline3 has an extra spline basis. basis_spline and basis_spline2 are the same (knots for basis_spline2 are from basis_spline)
basis_spline <- (ns(ucdp_onset_2014$peaceyears, df=4))
basis_spline2 <- (ns(ucdp_onset_2014$peaceyears, knots = c(2,15,32.75)))
basis_spline3 <- (ns(ucdp_onset_2014$peaceyears, knots = c(2,15,32.75), intercept = T))

#merge the ucdp dataset with the spline basis matrix
ucdp_onset_2014 <- cbind(ucdp_onset_2014,basis_spline)

#rename the spline base vectors and other columns for readability
ucdp_onset_2014 <- rename(ucdp_onset_2014, c("1"="pyspline_1", "2"="pyspline_2", "3"="pyspline_3", "4"="pyspline_4",
                                             "gwno"= "cowcode", "incidencev414"= "v_incidence_nl", "incidencev414-1"="v_incidence", "newconflictinyearv414"= "conflict_onset_ucdp",
                                             "onset2v414"="v_incidence2yr"))

#change name for merge
ucdp_final <- ucdp_onset_2014

#looks good


```

```{r}
#WDI dataset: Inflation and women in parliament (%)
  #merged with Paxton's dataset for WIP for the earlier years (WDI data has spotty observation before the early 2000s)


wdi_orginal <- read_csv("WDI_Data_1960-2017.csv")
wdi <- wdi_orginal
wdi[wdi==".."] <- NA #label NAs correctly
wdi$`Series Code` <- NULL #series code is redundant


#we need to remove the last 5 empty rows
nwdi<-dim(wdi)[1] #get the number of rows
wdi<-wdi[1:(nrow(wdi)-5),]


#changing year column names to just contain the years in numeric form
names(wdi)[-(1:3)] = gsub( ".{8}$" , replacement = "", x = names(wdi)[-(1:3)])

#rearranging data to tidy format
wdi <- gather(wdi, year, value, 4:61)
wdi <- spread(wdi, `Series Name`, value)

#change year to numeric
wdi$year <- as.numeric(wdi$year)

#dataset with the years we are interested in
wdi<- wdi[wdi$year %in% years, ]


#here countries are coded by ISO, while savun's original data uses COW
#we need to translate between the two
wdi_cowcode <- countrycode(wdi$`Country Code`, "iso3c", "cown", warn = TRUE)
wdi <- cbind(wdi_cowcode, wdi)

#before we just look at the recipients, I also make a separate dataset for the donors
wdi_donors <- wdi[wdi$`Country Code` %in% donors,] 
wdi_donors <- wdi_donors[wdi_donors$year %in% c(2003:2016),]
wdi_donors <- rename(wdi_donors, c("wdi_cowcode"="cowcode",
                                   "Proportion of seats held by women in national parliaments (%)" = "women_parliament"))
wdi_donors <- dplyr::select(wdi_donors, cowcode, year, women_parliament)
#creating cow codes for WDI and binding the cow codes to the WDI dataframe
#now we can choose just the countries we need
wdi <- wdi[wdi$wdi_cowcode %in% cow_code,] 
#creating a new dataset with the years and countries we are interested in

#let's see how many countries are included here
length(unique(wdi$`Country Code`))
#there are 192

#let's check which countries are missing
missing_ccode_wdi <- setdiff(cow_code, unique(wdi$wdi_cowcode))
#this gives me the names of the countries that are missing from Polity

#rename columns for better merging and readability

#now let's select just the variables we are interested in so that we can use it to build our dataset
#I will use the consumer price inflation until I find a reason to use another measure of inflation

#rename columns for better merging and readability
wdi_final <- rename(wdi, c("wdi_cowcode"="cowcode", 
                           "Inflation, consumer prices (annual %)" = "consumer_inflation",
                           "Proportion of seats held by women in national parliaments (%)" = "women_parliament",
                           "Country Name" = "country",
                           "Country Code" = "countrycode"))

#change women_parliament to numeric data
wdi_final$women_parliament <- as.numeric(wdi_final$women_parliament)

#this has both inflactio and WIP. I will separate the 2 later
wdi_both <- dplyr::select(wdi_final, cowcode, countrycode, country, year, consumer_inflation, women_parliament)

#looks good for the WDI part

#~~~~~~


#now to paxton data 1945-2003
paxton_wipdata <- read_dta("C:/Users/Spark/Desktop/Winter_2018/poli_666/Research/analysis/paxton_wipdata.dta")
paxton_wip <- paxton_wipdata

#label NAs correctly (-88 and -99 are missing data)
paxton_wip[paxton_wip=="-88"] <- NA
paxton_wip[paxton_wip=="-99"] <- NA

#we don't need the last dozen-ish columns after the years
paxton_wip <- paxton_wip[,1:which(colnames(paxton_wip)=="P2003")]

#we also don't need some columns in the beginning
paxton_wip <- paxton_wip[ , !(colnames(paxton_wip) %in% c("CASEID", "REGIONFULL", "REGIONCONCISE"))]


##we need to make the years to a numeric value and rearrange to country-year format
#first, fix the year values (remove the character "p" in all the column names)
paxtonyrname <- (which(colnames(paxton_wip)=="P1945"): which(colnames(paxton_wip)=="P2003"))

names(paxton_wip)[-(1:(min(paxtonyrname)-1))] <- gsub( "P", replacement = "",
                                                            x = names(paxton_wip)[-(1:(min(paxtonyrname)-1))])

#then, rearrange the structure
paxton_wip <- gather(paxton_wip, year, women_parliament, paxtonyrname)

#name year a numeric value
paxton_wip$year <- as.numeric(paxton_wip$year)

#dataset with the years we are interested in
#since we will merge this with WDI data, we end at from 2002
paxtonyears <- c(1989:2002)

paxton_wip<- paxton_wip[paxton_wip$year %in% paxtonyears, ]


#here countries are coded by UNID, while savun's original data uses COW
#we need to translate between the two
paxton_cowcode <- countrycode(paxton_wip$UNID, "un", "cown", warn = TRUE)
paxton_wip <- cbind(paxton_cowcode, paxton_wip)
#creating cow codes for paxton and binding the cow codes to the WDI dataframe

#before we just extract the recipients, we get the donor data for the IV's
donors_cowcode <- countrycode(donors, "iso3c", "cown", warn = TRUE)
paxton_donors <- paxton_wip[paxton_wip$paxton_cowcode %in% donors_cowcode, ]
paxton_donors <- rename(paxton_donors, c("paxton_cowcode"="cowcode"))
paxton_donors <-  dplyr::select(paxton_donors, cowcode, year, women_parliament)

#now we can choose just the countries we need
paxton_wip <- paxton_wip[paxton_wip$paxton_cowcode %in% cow_code,] 
#creating a new dataset with the years and countries we are interested in

#let's see how many countries are included here
length(unique(paxton_wip$paxton_cowcode))
#there are 161, which means that  18 countries are missing in the WDI data

#let's check which countries are missing
missing_ccode_paxton <- setdiff(cow_code, unique(paxton_wip$paxton_cowcode))
#this gives me the names of the countries that are missing from Polity

#rename the cowcode column
paxton_final <- rename(paxton_wip, c("paxton_cowcode"="cowcode"))


#merge WDI and Paxton
#I think it will be best if I just make two datasets, one for inflation and one for WIP

wdi_wip <- dplyr::select(wdi_both, cowcode, year, women_parliament)

#for wdi, we need to select only years from 2003-2016, and only the countries in both datasets
wdiwipyears <- c(2003:2016)
wdi_wip<- wdi_wip[wdi_wip$year %in% wdiwipyears, ]

#paxton wip data
paxton_final <-  dplyr::select(paxton_final, cowcode, year, women_parliament)

#WDI inflation data
inflation_final <- dplyr::select(wdi_both, cowcode, year, consumer_inflation)

#now we can bind all the wip data
combined_wip <- bind_rows(wdi_wip, paxton_final)

#order by country and year
combined_wip <- arrange(combined_wip, cowcode, year)

#and also only select countries in both datasets (so that I can easily make donor wip column)
wip_cowcode <- intersect(unique(wdi_wip$cowcode), unique(paxton_final$cowcode))
combined_wip <- combined_wip[combined_wip$cowcode %in% wip_cowcode, ]

#check the number of countries
length(unique(combined_wip$cowcode))
#there are 188 countries 

#~~~~
#lastly, donor wip can also be calculated through wip data
#we need the logged average of annual sweden,us,and france 1989-2014 (then repeat the data)

#wdi_donors$women_parliament <- as.numeric(wdi_donors$women_parliament)

#create a separate dataframe just for the donors
#donor_wip <- bind_rows(paxton_donors, wdi_donors)

#rearrange and calculate averages
#donor_wip <- spread(donor_wip, cowcode, women_parliament)
#donor_wip$donor_wip <- log(rowMeans(donor_wip[,-1]))

#we use the resulting vector to attach it to the pwt dataset
#donor_wip_avg <- rep(donor_wip$donor_wip, 161)
#repeating the vector 146 (while keeping the order)
#combined_wip <- cbind(combined_wip, donor_wip_avg)
#~~

#change name for merge
wip_final <- combined_wip


```



###### merge everything and export

```{r}
#merge all the data

#extended_data <- merge(ucdp_final, democracy_aid_final, by=c("cowcode","year"), all=TRUE) 

extended_data <- merge(ucdp_final, polity_2016_final, by=c("cowcode","year"), all=TRUE) 
extended_data <- merge(extended_data, pennwt_9_final, by=c("cowcode","year"), all=TRUE) 
#extended_data <- merge(extended_data, affinity_final, by=c("cowcode","year"), all=TRUE)
extended_data <- merge(extended_data, inflation_final, by=c("cowcode","year"), all=TRUE)
extended_data <- merge(extended_data, wip_final, by=c("cowcode","year"), all=TRUE)


#rename some variables
extended_data <- rename(extended_data, c("donor_gdp_vector"="donor_gdp"))

#also add regions 
extended_data$region <- countrycode(extended_data$cowcode, "cown", "region", warn = TRUE)

#add country name based on cowcode
extended_data$country_name <- countrycode(extended_data$cowcode, "cown", "country.name", warn = TRUE)


#moving country_name to the first column for better viewing
extended_data <- extended_data[,c(ncol(extended_data),1:(ncol(extended_data)-1))]

#remove some extra columns (country names)
extra_columns <- c("scode", "country.x", "countrycode", "country.y", "cyear", "recipient_country")
extended_data <- extended_data[ , !(names(extended_data) %in% extra_columns)]


#lead year by one year to have the effect of lagging all other variables
extended_data<- slide(extended_data, Var = "year", GroupVar = "cowcode", slideBy = 1)

#lead conflict_ini by one year to have the effect of lagging all other variables
#extended_data<- slide(extended_data, Var = "conflict_ini", GroupVar = "cowcode", slideBy = 1)

#do the same to other measures of conflict initiations
#extended_data<- slide(extended_data, Var = "v_incidence2yr", GroupVar = "cowcode", slideBy = 1)
#extended_data<- slide(extended_data, Var = "conflict_onset_ucdp", GroupVar = "cowcode", slideBy = 1)

#rearrange order of data
#extended_data <- extended_data[,c(1:7, ncol(extended_data)-2,ncol(extended_data)-1, ncol(extended_data),8:(ncol(extended_data)-3))]

#add dummy variable for after 2002
#extended_data$dummy2002 <- 0
#extended_data$dummy2002[which(extended_data$year >= 2002)] <- 1

#forgot to add t,t^2,t^3, I divided the cubed term by 1000 to avoid numeric instability as recommended by carter
extended_data$py_squared <- extended_data$peaceyears^2
extended_data$py_cubed <- extended_data$peaceyears^3*0.001


#I rename variables so that they are similar to the replication formula (new df just in case)
extended_data1 <- rename(extended_data, c("peaceyears"="peace_years", 
                                         "polity1_dem"="democratization1", "polity1_dem_ord" = "democratization1_ord",
                                         "polity"="democracy", "rgdppc_growth"="growth_real_gdppc", "rgdp_pc"="real_gdppc",
                                         "v_incidence" = "conflict_prioryear", "v_incidence_nl" ="conflict_prioryear_nl" ))

#and adjust the years to 1990-2015

years2 <- c(1990:2014)
extended_data1 <- extended_data1[extended_data1$year %in% years2,]


#looks good!


#let's export this to stata for use
write.dta(extended_data1, "C:/Users/Spark/Desktop/Winter_2018/ORGB_708/research/researchproj/extended_data_0401.dta")


```

END of ORGB 708 code




```{r}
##### Adding Controls
  # these ones are already lagged one year

controls <- read_dta("./raw_data/extended_data_0401.dta") %>%
  mutate(year = as.integer(year),
         cowcode = as.integer(cowcode))

#already lagged one year
#in country-year



###### Adding control variables to the dataset ######

###Recipient
#controls$conflict_prioryear
#controls$polity2_dem_ord #this is the change in democratization
#controls$democracy #this is the democracy score
#controls$pop #this is the population that has not been logged #I use it to scale the aid flow

###Both
#controls$growth_real_gdppc
#controls$real_gdppc
#controls$population

###Donor
#controls$consumer_inflation
#controls$women_parliament

#remember that pop and aidflows are both in millions

mergedata2 <- left_join(mergedata1,
                       select(controls,
                              cowcode,
                              year,
                              conflict_prioryear,
                              pop, #this is just used to correct the aid flow - this is in millions
                              polity2_dem_ord,
                              polity2,
                              growth_real_gdppc,
                              real_gdppc,
                              population), #"population"" is logged pop in thousands
                       by = c("ccode2" = "cowcode", 
                              "year"="year")) %>%
  rename(r_democracy = polity2,
         r_conflict_prioryear = conflict_prioryear,
         r_pop_notlogged = pop,
         r_politychange = polity2_dem_ord,
         r_gdp_growth = growth_real_gdppc,
         r_gdp = real_gdppc,
         r_population = population) %>%
  mutate(r_democratization = as.numeric(r_politychange > 2),
         r_gdplog = log(r_gdp),
         r_democracy_sq = (r_democracy)^2) %>%
  left_join(select(controls,
                   cowcode,
                   year,
                   pop,
                   growth_real_gdppc,
                   real_gdppc,
                   population,
                   consumer_inflation,
                   women_parliament),
            by = c("ccode1" = "cowcode", 
                   "year"="year")) %>%
  rename(d_gdp_growth = growth_real_gdppc,
         d_pop_notlogged = pop,
         d_gdp = real_gdppc,
         d_population = population,
         d_consumer_inflation = consumer_inflation,
         d_women_parliament = women_parliament) %>%
  mutate(d_gdplog = log(d_gdp)) %>%
  mutate_at(vars(contains("aid")), 
            .funs = funs(corr = log(((./r_pop_notlogged)*1000)+1))) %>%
  group_by(ccode2, ccode1) %>%
  mutate_at(vars(ends_with("corr")),
            .funs = funs(`1` = lag(., order_by = year))) %>%
  mutate_at(vars(ends_with("corr")),
            .funs = funs(`3` = lag(., order_by = year, n = 3))) %>%
  mutate_at(vars(ends_with("corr")),
            .funs = funs(`5` = lag(., order_by = year, n = 5))) %>%
  mutate_at(vars(starts_with("r_")),
            .funs = funs(`1` = lag(., order_by = year))) %>%
  mutate_at(vars(starts_with("d_")),
            .funs = funs(`1` = lag(., order_by = year)))

#NA created for aid corrected for population because of missing population data   
  
#sanity check - only very few are nans
#sapply(mergedata, function(x) sum(is.nan(x)))
#head(select(mergedata, democracy_aid_com_corr, democracy_aid_com_corr_l1)[100:106,])

#for 666, lag democratization by 3 - should I still do this?

write.csv(mergedata2, "mergedata_dec16.csv", row.names = F)

```


```{r}
###CEPII data for distance (no year) 
cepii <- read_dta("dist_cepii.dta") %>%
  mutate(ccode1 = countrycode(iso_o, "iso3c", "cown"),
         ccode2 = countrycode(iso_d, "iso3c", "cown"),
         distlog = log(dist),
         d_region = countrycode(ccode1, "cown", "region"),
         r_region = countrycode(ccode2, "cown", "region"),
         samereg = as.numeric(d_region == r_region)) 


#only the small countries (islands) are missing
#dist is the simple distance between the biggest cities
#colony is whether the pair of countries were ever in a colonial relationship
#distw and distwces are basically the same, but distwces is the inverse to distw
  #both are weighted distance
  #distwces shrinks as distance grows and distw is the opposite



```


### DESTA data- PTA 

```{r}
#create column for number of treaties to be filled with 0s too after merge (?- april 19)
#for some reason I only looked at PTAs prior to 2005 - idk why I did that, so I removed this restriction

DESTA <- read.csv(file="DESTA_treaties_dyadic.csv", stringsAsFactors = F)

desta1 <- select(DESTA, 
                 iso1, 
                 iso2, 
                 base_treaty,
                 year) %>%
  mutate(ccode1 = countrycode(iso1, "iso3n", "cown"),
         ccode2 = countrycode(iso2, "iso3n", "cown")) %>%
  filter(!(is.na(ccode1)) & !is.na(ccode2)) %>%
  distinct() %>%
  group_by(ccode1, ccode2, year) %>%
  summarise(ptacount = n()) 


desta_both <- rbind(desta1 %>%
                      ungroup(),
                    desta1 %>%
                      rename(c1 = ccode1,
                             c2 = ccode2) %>%
                      mutate(ccode1 = c2,
                             ccode2 = c1) %>%
                      ungroup() %>%
                      select(-c1, -c2)) %>%
  group_by(ccode1, ccode2, year) %>%
  summarise(ptacount = n()) %>%
  ungroup()

  

#ptacount equals 1 for both direction of the order of ccode1/2
  #in desta, each agreement is only counted once (if there is US-CAN, then there is no CAN-US for a particular agreement)

#because there are missing years, the cumulative value and lagged values need to be calculated after merge
```




### Unemployment Data 

```{r}
unemployment_worldbank <- read.csv(file="Unemployment_WB.csv", skip = 4, header=TRUE, sep=",") %>%
  select(-X)


#remove the "X" in front of all the column names for the years

unemployment <- unemployment_worldbank %>%
  select(-Indicator.Code,
         -Indicator.Name) %>% #we only picked one indicator so this is all the same
  rename_at(vars(starts_with("X")),
                 funs(substr(.,2, 5))) %>%
  gather(year, unemployment, -c(Country.Name, Country.Code)) %>%
  group_by(Country.Code) %>%
  mutate(year = as.integer(year),
         ccode = countrycode(Country.Code, "iso3c", "cown"),
         unemployment_1 = lag(unemployment, order_by = year)) %>% #lag 1 year
  ungroup()


#looks good
```



### Immigration Data 

```{r}
immigration_oecd <- read.csv("Migration_1975-2015.csv", stringsAsFactors = F)

#not scaled (count at single person level, not in millions, etc.)
#all host countries are OECD members

#filter only "inflow of foreign population by nationality" (to an OECD member), or VAR = B11
  #foreign population means people with another nationality

#DEC 12 : also filter B16 (Acquisition of nationality by country of former nationality)
  #this is naturalization data 

immigration <- filter(immigration_oecd, 
                      VAR %in% c("B11", "B16")) %>%
  select(`ï..CO2`,
         COU,
         Year,
         Value,
         VAR) %>% 
  spread(VAR, Value) %>%
  rename(country1 = COU, #donor (inflow)
         country2 = `ï..CO2`, #recipient (outflow)
         year = Year,
         immigration = B11,
         naturalization = B16) %>%
  mutate(ccode1 = countrycode(country1, "iso3c", "cown"),
         ccode2 = countrycode(country2, "iso3c", "cown")) %>%
  filter(!is.na(ccode1) & !is.na(ccode2)) %>%
  group_by(country1,
           country2) %>%
  mutate(immigration_1 = lag(immigration, order_by = year),
         immigration_3 = lag(immigration, 3, order_by = year),
         immigration_5 = lag(immigration, 5, order_by = year),
         immigration_7 = lag(immigration, 7, order_by = year),
         immigration_10 = lag(immigration, 10, order_by = year),
         naturalization_1 = lag(naturalization, order_by = year),
         naturalization_3 = lag(naturalization, 3, order_by = year),
         log_immigration = log(immigration + 1),
         log_immigration_1 = log(immigration_1 + 1),
         log_immigration_3 = log(immigration_3 + 1),
         log_immigration_5 = log(immigration_5 + 1),
         log_immigration_7 = log(immigration_7 + 1),
         log_immigration_10 = log(immigration_10 + 1),
         log_naturalization = log(naturalization + 1),
         log_naturalization_1 = log(naturalization_1 + 1),
         log_naturalization_3 = log(naturalization_3 + 1)) %>%
  ungroup()


#the country1 column includes TOT which means "total"- we should remove this


#I lag this 1, 3, and 5 years because there is an argument in Menard that immigration can lead to unemployment 
# which can lead to aid (I don't think this is directly related to lagging, but it will be good to check in case)


#Dec 12 - I also did a 7 year lag (used by Jones-Correa) and a 10 year lag to proxy for immigrants who have stayed for a while and are more likely to have been naturalized. 


#Dec 29 - need to check that all years are present and the lags actually work- it does, the lags are ok
 # - should use log_naturalization_lag


#summary(immigration$immigration)
#looks good


# corrplot(cor(select(immigration,
#                     immigration,
#                     naturalization), use = "complete.obs"),
#          method = "number")

#here the correlation between immigration and naturalization is not too correlated

#sanity check
#head(immigration, n = 10)

```



### Trade Data from IMF

```{r}
#I use exports like DESTA did
  #use export from oecd to recipient, and then recipient to oecd
#export in dollars

trade_imf <- read.csv(file="DOT_04-19-2018 19-48-46-85_panel.csv",
                      stringsAsFactors = F)

#select just the columns I need
trade <- select(trade_imf,
                Country.Code, 
                Counterpart.Country.Code, 
                Time.Period,
                Goods..Value.of.Exports..Free.on.board..FOB...US.Dollars..TXG_FOB_USD.) %>%
  rename(countryfrom = Country.Code,
         countryto = Counterpart.Country.Code,
         year = Time.Period,
         export = Goods..Value.of.Exports..Free.on.board..FOB...US.Dollars..TXG_FOB_USD.) %>%
  mutate(ccodefrom = countrycode(countryfrom, "imf", "cown"),
         ccodeto = countrycode(countryto, "imf", "cown")) %>% #change from IMF code
  filter(!is.na(ccodeto) & !is.na(ccodefrom))



```


### US military aid from foreign aid greenbook

```{r}

#in 2016 constant dollars, obligations not disbursement

us_military_aid <- read_excel("us_foreignaid_greenbook.xlsx", skip = 6) %>%
  rename(year = `Fiscal Year`,
         country_name = Country,
         category = `Assistance Category`,
         military_aid = `Obligations (Constant Dollars)`) %>%
  filter(category == "Military") %>%
  select(year,
         country_name,
         military_aid) %>%
  mutate(ccode2 = countrycode(country_name, "country.name", "cown")) %>%
  filter(!is.na(ccode2)) %>%
  group_by(year,
           ccode2) %>%
  summarise(military_aid = sum(military_aid)) %>%
  ungroup() %>%
  group_by(ccode2) %>%
  complete(year = 1947:2016) %>%
  mutate(military_aid = ifelse(is.na(military_aid), 0, military_aid))



```




Merging part 3- final

```{r}


mergedata3 <- left_join(mergedata2,
                        select(cepii,
                               contig,
                               comlang_off,
                               comlang_ethno,
                               colony,
                               dist,
                               distw,
                               distwces,
                               ccode1,
                               ccode2,
                               distlog, #this is the log of simple distance
                               d_region,
                               r_region,
                               samereg),
                        by = c("ccode1",
                               "ccode2")) %>%
  left_join(desta_both,
            by = c("ccode1",
                   "ccode2",
                   "year")) %>%
  group_by(ccode1, ccode2) %>%
  arrange(ccode1, ccode2, year) %>%
  mutate(ptacount = ifelse(is.na(ptacount), 0, ptacount), #added up duplicates already
         ptasum = cumsum(ptacount), # Cumulative count of treaties - will need to autocorrelation
         ptasum_1 = lag(ptasum, order_by = year),
         ptacount_1 = lag(ptacount, order_by = year)) %>%
  left_join(select(unemployment, #unemployment columns for recipient
                   ccode,
                   year,
                   unemployment,
                   unemployment_1),
            by = c("ccode2" = "ccode",
                   "year")) %>%
  rename(r_unemployment = unemployment,
         r_unemployment_1 = unemployment_1) %>%
  left_join(select(unemployment, #unemployment columns for donors
                   ccode,
                   year,
                   unemployment,
                   unemployment_1),
            by = c("ccode1" = "ccode",
                   "year")) %>%
  rename(d_unemployment = unemployment,
         d_unemployment_1 = unemployment_1) %>%
  mutate(unempdiff = d_unemployment - r_unemployment, #also a column for unemployment differences
         unempdiff_1 = lag(unempdiff, order_by = year)) %>% #negative value: donor has less unemployment
  left_join(select(immigration,
                   -country1,
                   -country2),
            by = c("ccode1",
                   "ccode2",
                   "year")) %>% #adding a 5-year rolling sum of naturalization
  mutate(naturalization_roll = roll_sum(naturalization, 5, fill = NA, align = "right"),
         log_naturalization_roll = log(roll_sum(naturalization, 5, fill = NA, align = "right")+1),
         log_naturalization_roll_1 = lag(log_naturalization_roll, order_by = year),
         immigration_roll = roll_sum(immigration, 5, fill = NA, align = "right"),
         log_immigration_roll = log(roll_sum(immigration, 5, fill = NA, align = "right")+1),
         log_immigration_roll_1 = lag(log_immigration_roll, order_by = year)) %>%
  left_join(select(trade, #need to merge separately to get d to r and r to d exports
                   ccodeto,
                   ccodefrom,
                   year,
                   export),
            by = c("ccode1" = "ccodefrom",
                   "ccode2" = "ccodeto",
                   "year")) %>%
  rename(dtor_export = export) %>%
  left_join(select(trade, #need to merge separately to get d to r and r to d exports
                   ccodeto,
                   ccodefrom,
                   year,
                   export),
            by = c("ccode2" = "ccodefrom",
                   "ccode1" = "ccodeto",
                   "year")) %>%
  rename(rtod_export = export) %>%
  mutate(log_rtod_export = log(rtod_export+1),
         log_dtor_export = log(dtor_export+1),
         rtod_export_1 = lag(rtod_export, order_by = year),
         dtor_export_1 = lag(dtor_export, order_by = year),
         log_rtod_export_1 = lag(log_rtod_export, order_by = year),
         log_dtor_export_1 = lag(log_dtor_export, order_by = year)) %>%
  left_join(us_military_aid,
            by = c("ccode2",
                   "year")) %>%
  mutate(military_aid_corr = log((military_aid/r_pop_notlogged*1000)+1),
         military_aid_corr_1 = lag(military_aid_corr, order_by = year),
         military_aid_corr_3 = lag(military_aid_corr, 3, order_by = year),
         military_aid_corr_5 = lag(military_aid_corr, 5, order_by = year),
         dyad_id = paste(ccode1, ccode2, sep = "_")) #creating a dyad-id for augmented dickey fuller tests


######################## DONE with merging ######################
#################################################################


###to do TWC with HC3 errors, turn data into pdata.frame and use plm
mergepdata <- pdata.frame(mergedata3, index = c("ccode1", "ccode2"))

#there are warning because of duplicate dyad (since I have years now)
#let's proceed for now and see what happens

#sanity check
head(mergedata3[200:210,c(1,2,3,133,134,142,148,151)], n = 10)


```





### Corrections, other cleaning tasks


```{r}
########## I need to remove all missing aidflow corrected value#######
  #since we can't have missing Y

# mergedata <- filter(mergedata, 
#                      !is.na(aidflow_corrected) & !is.nan(aidflow_corrected) & !is.infinite(aidflow_corrected))
  # I would not do this because we still need to lag data


######### I also need to double lag all variables and get the FD for ECM  ########
  #ipfd would be dropped and added as fd
  

#for testing for stationarity and first difference model

mergedata4 <- mergedata3 %>%
  group_by(ccode1, ccode2) %>%
  mutate(naturalization_prop_log = log(naturalization/(d_pop_notlogged)+1), #proportional to donor pop in mil
         immigration_prop_log = log(immigration/(d_pop_notlogged)+1),
         naturalization_roll_prop_log = log(naturalization_roll/(d_pop_notlogged)+1),
         immigration_roll_prop_log = log(immigration_roll/(d_pop_notlogged)+1)) %>%
  mutate_all(.funs = funs(lag = lag(., order_by = year))) %>%
  select(-dyad_id_lag) %>%
  mutate(democracy_aid_com_corr_df = democracy_aid_com_corr - democracy_aid_com_corr_lag,
         democracy_aid_disb_corr_df = democracy_aid_disb_corr - democracy_aid_disb_corr_lag,
         democracy_aid_com_corr_df_lag = lag(democracy_aid_com_corr_df, order_by = year),
         democracy_aid_disb_corr_df_lag = lag(democracy_aid_disb_corr_df, order_by = year),
         disaster_aid_com_corr_df = disaster_aid_com_corr - disaster_aid_com_corr_lag,
         econ_capacity_aid_com_corr_df = econ_capacity_aid_com_corr - econ_capacity_aid_com_corr_lag,
         other_aid_com_corr_df = other_aid_com_corr - other_aid_com_corr_lag,
         production_aid_com_corr_df = production_aid_com_corr - production_aid_com_corr_lag,
         program_aid_com_corr_df = program_aid_com_corr - program_aid_com_corr_lag,
         social_aid_com_corr_df = social_aid_com_corr - social_aid_com_corr_lag,
         disaster_aid_disb_corr_df = disaster_aid_disb_corr - disaster_aid_disb_corr_lag,
         econ_capacity_aid_disb_corr_df =econ_capacity_aid_disb_corr - econ_capacity_aid_disb_corr_lag,
         other_aid_disb_corr_df = other_aid_disb_corr - other_aid_disb_corr_lag,
         production_aid_disb_corr_df = production_aid_disb_corr - production_aid_disb_corr_lag,
         program_aid_disb_corr_df = program_aid_disb_corr - program_aid_disb_corr_lag,
         social_aid_disb_corr_df = social_aid_disb_corr - social_aid_disb_corr_lag,
         r_conflict_prioryear_df = r_conflict_prioryear - r_conflict_prioryear_lag, #I skip ipdiff bc it is already fd
         r_politychange_df = r_politychange - r_politychange_lag,
         r_democracy_df = r_democracy - r_democracy_lag,
         r_gdp_growth_df = r_gdp_growth - r_gdp_growth_lag,
         r_gdp_df = r_gdp - r_gdp_lag,
         r_population_df = r_population - r_population_lag,
         r_democratization_df = r_democratization - r_democratization_lag,
         r_gdplog_df = r_gdplog - r_gdplog_lag,
         r_democracy_sq_df = r_democracy_sq - r_democracy_sq_lag,
         d_gdp_growth_df = d_gdp_growth - d_gdp_growth_lag,
         d_gdp_df = d_gdp - d_gdp_lag,
         d_population_df = d_population - d_population_lag,
         #d_consumer_inflation_df = d_consumer_inflation - d_consumer_inflation_lag,  # this a chr vector
         d_women_parliament_df = d_women_parliament - d_women_parliament_lag,
         d_gdplog_df = d_gdplog - d_gdplog_lag,
         ptacount_df = ptacount - ptacount_lag,
         ptasum__df = ptasum - ptasum_lag,
         unempdiff_df = unempdiff - unempdiff_lag,
         log_immigration_df = log_immigration - log_immigration_lag,
         log_immigration_5_df = log_immigration_5 - log_immigration_5_lag,
         log_naturalization_df = log_naturalization - log_naturalization_lag,
         log_naturalization_1_df = log_naturalization_1 - log_naturalization_1_lag,
         log_naturalization_3_df = log_naturalization_3 - log_naturalization_3_lag,
         log_naturalization_roll_df = log_naturalization_roll - log_naturalization_roll_lag,
         log_immigration_roll_lag = log_immigration_roll - log_immigration_roll_lag,
         log_rtod_export_df = log_rtod_export - log_rtod_export_lag,
         log_dtor_export_df = log_dtor_export - log_dtor_export_lag,
         military_aid_df = military_aid - military_aid_lag,
         military_aid_corr_df = military_aid_corr - military_aid_corr_lag,
         other_economic_aid = log(((disaster_aid_disb +
                                    econ_capacity_aid_disb +
                                    other_aid_disb + 
                                    production_aid_disb +
                                    program_aid_disb +
                                    social_aid_disb)/1000)+1),
         other_economic_aid_lag = lag(other_economic_aid, order_by = year),
         other_economic_aid_df = other_economic_aid - other_economic_aid_lag,
         naturalization_prop_log_df = naturalization_prop_log - naturalization_prop_log_lag,
         immigration_prop_log_df = immigration_prop_log - immigration_prop_log_lag,
         naturalization_roll_prop_log_df = naturalization_roll_prop_log - naturalization_roll_prop_log_lag,
         immigration_roll_prop_log_df = immigration_roll_prop_log - immigration_roll_prop_log_lag) %>%
  mutate_at(vars(ends_with("_df")),
            .funs = funs(lag = lag(., order_by = year)))

#str(mergedata4[ , 250:338])
 #I should lag all the dfs (DONE)

```







```{r}

# ###no iraq data
# #remove the US and Iraq (or just Iraq)
# mergepdata_noirq <- filter(mergepdata, ccode2 != 625 )
# #countrycode("Iraq", "country.name", "cown")
# 
# #looks good
# 
# 
# #let's export this as *.dta for later use
#   # we need to take care of factors before converting to dta
# 
# mergepdata$iso_o <- as.character(mergepdata$iso_o)
# mergepdata$iso_d <- as.character(mergepdata$iso_d)
# 
# mergepdata$region1 <- as.character(mergepdata$region1)
# mergepdata$region2 <- as.character(mergepdata$region2)
# 
# mergepdata$continent1 <- as.character(mergepdata$continent1)
# mergepdata$continent2 <- as.character(mergepdata$continent2)
# 
# class(mergepdata$region1)
# 
# table(sapply(mergepdata, is.factor))
# 
# write.dta(mergepdata, "C:/Users/Spark/Desktop/Winter_2018/ORGB_708/research/researchproj/708data_0421.dta", 
#           convert.factors = "numeric")
# 
# 
# #added oct 6 2018
# aa <- read_dta("708data_0421.dta")
# 
# write.csv(aa, "UNGA_fulldata.csv", row.names = F)


```






